[PATH]
train = data/train_val_test/fewer_labels/train.json
val = data/train_val_test/fewer_labels/val.json
test = data/train_val_test/fewer_labels/test.json
responses_text = data/responses/output.json
resp_ids_by_label = data/responses/ids_by_label.json
encoded_responses = data/responses/encoded_responses.npy
vectorized_train = data/train_val_test/train.npy
vectorized_val = data/train_val_test/val.npy
algo_midas = models/v3/ml/MidasRF.joblib
algo_entity = models/v3/ml/EntityLR.joblib
tfidf = models/v3/tfidf/tfidf.pkl
bert_midas = models/v3/bert/midas
bert_entity = models/v3/bert/entity
output_dir = data/predictions

[VAR]
context_len = 3
n_epochs = 5
max_length = 256

[URL]
use = https://tfhub.dev/google/universal-sentence-encoder/4
convert = http://files.deeppavlov.ai/alexaprize_data/convert_reddit_v2.3.punct.tar.gz
bert = bert-base-cased
roberta = roberta-base

[CONVERT]
text_context = data/convert_tests/val_context.json
encoded_context = data/convert_tests/val_context.npy
y_true = data/convert_tests/y_val.json
responses = data/convert_tests/responses_all.json
val_responses = data/convert_tests/val_responses.json
encoded_responses = data/convert_tests/responses_all.npy

[RANKER]
responses = data/ranking/responses_for_inference.json
encoded_responses = data/ranking/encoded_responses_for_inference.npy
tfidf_responses = data/ranking/responses_tfidf.npz

[DATASET]
daily = data/datasets/daily_dialogue_annotated_v3.json
topical = data/datasets/topical_chat_annotated_v3.json
