import math
import numpy as np
import tensorflow as tf

from tensorflow.keras.utils import Sequence

class SkillDataset(Sequence):
    
    """ customized Dataset class from torch """
    
    def __init__(
        self, data: list, vars2id: dict, 
        text_vectorizer, label_encoder,
        n_previous: int=3, embed_dim: int = 512, 
        batch_size: int = 32, shuffle: bool = False):
        
        self.data = data
        self.indexes = np.arange(len(self.data))
        self.vars2id = vars2id
        self.vectorizer = text_vectorizer
        self.label_encoder = label_encoder
        self.n_previous = n_previous
        self.utterance_dim = (
            embed_dim + 
            len(vars2id['midas2id']) + 
            len(vars2id['entities2id']))
        
        self.batch_size = batch_size
        self.shuffle=shuffle
        
    def __len__(self):
        """
        Denotes the number of batches per epoch
        A common practice is to set this value to [num_samples / batch sizeâŒ‹
        so that the model sees the training samples at most once per epoch.
        """
        return int(np.ceil(len(self.data) / self.batch_size))

    
    def on_epoch_end(self):
        """
        Updates indexes after each epoch
        Shuffling the order so that batches between epochs do not look alike.
        It can make a model more robust.
        """
        if self.shuffle:
            np.random.shuffle(self.indexes)
    
    def __getitem__(self, idx: int):
        """ get batch_id and return its vectorized representation """
        indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch = [self.data[index] for index in indexes]
        
        x_batch, y_batch = self.__extract_features(batch)
        
        return x_batch, y_batch
    
    
    def __extract_features(self, batch) -> tuple:
        """ 
        transaforms raw data into vectorized features and encoded labels
        and collate them into batches
        """
        x_batch = np.zeros([len(batch), self.n_previous*self.utterance_dim])
        y_batch = list()
        
        for i, sample in enumerate(batch):
            embedding = self.__embed(sample['previous_text'])
            x_midas = self.__norm_midas(sample['midas_vectors'])
            x_entities = self.__oh_encode(sample['previous_entities'])
            x_batch[i,:] = self.__concat_vecs(embedding, x_midas, x_entities)
            y_batch.append(
                (sample['predict']['midas'], sample['predict']['entity']['label'])
            )
            
        y_batch = self.label_encoder.to_categorical(y_batch)
            
        return x_batch, y_batch
            

    def __embed(self, utterances: list) -> np.ndarray:
        """ 
        vectorizes a list of N previous utterances using a provided encoder
        
        USE returns Tensors but item assignment is performed downstream,
        thus it is converted to numpy as EagerTensor objects 
        do not support item assignment
        
        input: List[str]
        output: numpy array (len(utterance), embed_dim)
        """
        return self.vectorizer(utterances).numpy()
    
    def __norm_midas(self, midas_vectors: list) -> np.ndarray:
        """ 
        takes midas vectors of all sentences in the utterance
        and returns a vector with max values per midas label
        """
        vecs = np.zeros((len(midas_vectors), 13))
        
        for i, vec in enumerate(midas_vectors):
            # get max probability per each midas labels
            vecs[i] = np.max(np.array(vec), axis=0)

        # return normalized
        return vecs

    def __oh_encode(self, entities) -> np.ndarray:
        """ 
        one-hot encoding of entities per each sample 
        
        TODO: replace with sklearn MultiLabelBinarizer
        """
        
        entities = [[ent['label'] for ent in ut] for ut in entities]
        ohe_vec = np.zeros((len(entities), len(self.vars2id['entities2id'])))
        
        for i, ut in enumerate(entities):
            for ent in set(ut):
                ohe_vec[i][self.vars2id['entities2id'][ent]] = 1
                
        return ohe_vec
    
      
    def __concat_vecs(self, embedding: tf.Tensor,
                      midas_vec: np.array, 
                      ohe_vec: np.array) -> tf.Tensor:
        """ 
        concatenates text embeddings with midas vectors 
        and one-hot encoded entities
        
        The output vector will be (n_utterances, self.vector_dim)
        Vector dim comes from:
        1. [tfidf utterance(i-2)]
        2. [midas proba distribution utterance(i-2)]
        3. [entity type one-hot utterance(i-2)]
        4. [tfidf (i-1)]
        5. [midas (i-1)][entity (i-1)]
        6. [tfidf (i)] 
        7. [midas (i)]
        8. [entity (i)]
        """
        assert embedding.shape[0] == midas_vec.shape[0] == ohe_vec.shape[0]
        
        vecs = np.zeros((self.n_previous, self.utterance_dim))
        
        vecs[:,:embedding.shape[1]] = embedding
        vecs[:,embedding.shape[1]:embedding.shape[1]+midas_vec.shape[1]] = midas_vec
        vecs[:,embedding.shape[1]+midas_vec.shape[1]:] = ohe_vec

        # returned one utterance vectors shaped from its sentences
        return tf.reshape(vecs, [-1])