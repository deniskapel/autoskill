{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b300286-99df-4f4a-8014-f7ec3100c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.data2seq import Dial2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035146f8-c020-4531-96cd-e0837a3afb7a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fba64-2300-42b0-aaa1-22090880724c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e32f9a3-db24-4691-8f5d-96a6f48ad067",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/daily_dialogue_annotated_update.json', 'r', encoding='utf8') as f:\n",
    "    daily = json.load(f)\n",
    "    \n",
    "with open('data/topical_chat_annotated_update.json', 'r', encoding='utf8') as f:\n",
    "    topical = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e417c9-1010-4252-be46-2428caa57217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Length in dialogues: topical chat - 8628, daily dialog - 12376'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Length in dialogues: topical chat - {len(topical)}, daily dialog - {len(daily)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3157f2ec-08c6-43be-b38a-0031b2ac187f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = list(topical.items()) + list(daily.items())\n",
    "len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f22e7c98-4c78-4ab5-9717-66fd94f3167b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16803, 3701, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val_test = train_test_split(total, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(val_test, test_size=500, random_state=42)\n",
    "\n",
    "train, val, test = dict(train), dict(val), dict(test)\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57efb55-a115-4af3-a717-0699333e5ba5",
   "metadata": {},
   "source": [
    "save data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e56007-5a68-41ce-b2cd-ddf0926ce099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of dialogues\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12376, 8628)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7398625-b309-4050-a9c1-2e5dd12665e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/train_dialogues.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "480ebe79-636e-44bf-8aff-c965c8bf65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/val_dialogues.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e0517ed-99e7-442a-ba4a-990ff0815d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/test_dialogues.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d730b-6ac3-4fd2-9c71-aeaa969a511a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66cc168c-a970-4e56-9e98-eeeb9de53308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences in the Train dataset is  179286\n",
      "Total number of sequences in the Validation dataset is 39089\n",
      "Total number of sequences in the Test dataset is 5206\n"
     ]
    }
   ],
   "source": [
    "train_seqs = Dial2seq('data/annotated/train_dialogues.json', 3).transform()\n",
    "print('Total number of sequences in the Train dataset is ', len(train_seqs))\n",
    "val_seqs = Dial2seq('data/annotated/val_dialogues.json', 3).transform()\n",
    "print('Total number of sequences in the Validation dataset is', len(val_seqs))\n",
    "test_seqs = Dial2seq('data/annotated/test_dialogues.json', 3).transform()\n",
    "print('Total number of sequences in the Test dataset is', len(test_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e75998-3eeb-46cf-a328-828d6eabd162",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "249987ee-7798-42a3-bd10-a652eabd187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Validator(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_valid(self, seq: dict):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class OneEntity(Validator):\n",
    "    \n",
    "    def __init__(self, stoplist: list = ['misc', 'anaphor']):\n",
    "        self.stoplist = stoplist\n",
    "    \n",
    "    def is_valid(self, seq:dict) -> bool:\n",
    "        \"\"\"\n",
    "        checks if the first sentence of the sequence has\n",
    "        one annotated entity and it is not in the stoplist\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, stoplist: list = ['misc', 'anaphor']):\n",
    "            self.stoplist = stoplist\n",
    "\n",
    "        if len(seq['entities'][0]) != 1:\n",
    "            return False\n",
    "        \n",
    "        return seq['entities'][0][0]['label'] not in self.stoplist\n",
    "    \n",
    "    \n",
    "class NoEntity(Validator):\n",
    "    \n",
    "    def is_valid(self, seq:dict) -> bool:\n",
    "        \"\"\"\n",
    "        checks if the first sentence of the sequence has\n",
    "        one annotated entity and it is not in the stoplist\n",
    "        \"\"\"\n",
    "        return len(seq['entities'][0]) == 0\n",
    "        \n",
    "    \n",
    "    \n",
    "class SequencePreprocessor():\n",
    "    \"\"\" \n",
    "    preprocesses sequences\n",
    "    to filter only those that are relevant for the task\n",
    "    \n",
    "    params:\n",
    "    num_entities: int - maximum size of a last sentence in a sequence \n",
    "    in terms of number of annotated entities \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stoplist_labels: list = ['misc', 'anaphor'],\n",
    "                 seq_validator=None):\n",
    "        self.stoplist_labels = stoplist_labels\n",
    "        self.seq_validator = seq_validator\n",
    "        \n",
    "    def transform(self, sequences: list) -> list:\n",
    "        \"\"\" extract only necessary data from sequences \"\"\"\n",
    "        seqs = list()\n",
    "        \n",
    "        for seq in sequences:\n",
    "            if self.seq_validator and not self.seq_validator.is_valid(seq[-1]):\n",
    "                # validate final utterance if necessary\n",
    "                continue\n",
    "            sample = self.__get_dict_entry(self.__shape_output(seq))\n",
    "            seqs.append(sample)\n",
    "            \n",
    "        return seqs\n",
    "    \n",
    "\n",
    "    def __shape_output(self, seq: list) -> list:\n",
    "        \"\"\" shapes sequence in order to keep only the necessary data \"\"\"\n",
    "        output = list()\n",
    "        \n",
    "        # preprocess context\n",
    "        for ut in seq[:-1]:\n",
    "            midas_labels, midas_vectors = self.__get_midas(ut['midas'])\n",
    "            output.append((\n",
    "                ut['text'], midas_labels, midas_vectors, ut['entities']))\n",
    "\n",
    "        # preprocess target: only the first sentence of \n",
    "        # the last utterance in the sequence\n",
    "        midas_labels, midas_vectors = self.__get_midas(seq[-1]['midas'])\n",
    "        midas_labels, midas_vectors = midas_labels[0:1], midas_vectors[0:1]\n",
    "        sentence = seq[-1]['text'][0].lower()\n",
    "        entities = seq[-1]['entities'][0]\n",
    "        \n",
    "        if entities:\n",
    "            # filter out labels from stoplist\n",
    "            entities = [e for e in entities if e['label'] not in self.stoplist_labels]\n",
    "            # pre-sort them -> longest first to prevent mess with overlapping entities\n",
    "            entities = sorted(entities, key=lambda x: len(x['text']), reverse=True) \n",
    "        \n",
    "        ## replace entities with their labels\n",
    "        for ent in entities:\n",
    "            sentence = sentence.replace(ent['text'], ent['label'].upper())\n",
    "            \n",
    "        output.append(\n",
    "            (sentence, midas_labels[0], entities))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def __get_dict_entry(self, seq) -> dict:\n",
    "        \"\"\" creates a proper dict entry to dump into a file \"\"\"\n",
    "        entry = dict()\n",
    "        entry['previous_text'] = [s[0] for s in seq[:-1]]\n",
    "        entry['previous_midas'] = [s[1] for s in seq[:-1]]\n",
    "        entry['midas_vectors'] = [s[2] for s in seq[:-1]]\n",
    "        entry['previous_entities'] = [s[-1] for s in seq[:-1]]\n",
    "        entry['predict'] = {}\n",
    "        entry['predict']['text'] = seq[-1][0]\n",
    "        entry['predict']['midas'] = seq[-1][1]\n",
    "        entry['predict']['entities'] = seq[-1][2]\n",
    "        \n",
    "        return entry\n",
    "            \n",
    "        \n",
    "    def __get_midas(self, midas_labels: list) -> tuple:\n",
    "        \"\"\" \n",
    "        extracts midas labels with max value per each sentence in an utterance\n",
    "        and return a midas vector per each sentence\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        vectors = []\n",
    "        \n",
    "        for sentence_labels in midas_labels:\n",
    "            labels.append(max(sentence_labels, key=sentence_labels.get))\n",
    "            vectors.append(list(sentence_labels.values()))\n",
    "            \n",
    "        return labels, vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f9521-ba43-48f5-94ff-d133518aece8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocess targets for Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "33ba53ce-7ae5-46b1-bdcd-912dba92cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_convert = SequencePreprocessor(seq_validator=OneEntity())\n",
    "targets = for_convert.transform(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "306b2c30-fe21-4d47-81c9-d47f2d25447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_convert = SequencePreprocessor(seq_validator=NoEntity())\n",
    "targets_zero = for_convert.transform(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "811dd5b9-77ec-4784-afd1-45f9f3c97c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9991, 81103)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets), len(targets_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "a7befca7-d846-446b-abba-a3e48a2b4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, targets_zero = train_test_split(targets_zero, test_size=len(targets), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5e5a1dd4-81a1-41cd-ae88-20608ee22296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9991"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "cdf5d5e4-8927-4ba6-8d8b-974e06f1dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_contexts, convert_responses = list(), list()\n",
    "\n",
    "midas_counter = Counter()\n",
    "entity_counter = Counter()\n",
    "midas_entity_counter = Counter()\n",
    "\n",
    "for seq in targets:\n",
    "    convert_contexts.append(seq['previous_text'])\n",
    "    \n",
    "    midas = seq['predict']['midas']\n",
    "    entity = seq['predict']['entities']\n",
    "    entity = entity[0]['label'] if entity else None\n",
    "    text = seq['predict']['text']\n",
    "    convert_responses.append((midas,entity,text))\n",
    "    \n",
    "    # calc stats\n",
    "    midas_counter.update([midas])\n",
    "    entity_counter.update([entity])\n",
    "    midas_entity_counter.update([f'{midas} {entity}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "13bd2236-6a61-47c5-b0e1-0b204ddb7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/convert_responses.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(convert_responses, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "5e894915-e602-4bf7-854f-5144fa5b6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_contexts, zero_responses = list(), list()\n",
    "\n",
    "zero_midas_counter = Counter()\n",
    "\n",
    "for seq in targets_zero:\n",
    "    zero_contexts.append(seq['previous_text'])\n",
    "    \n",
    "    midas = seq['predict']['midas']\n",
    "    entity = None\n",
    "    text = seq['predict']['text']\n",
    "    zero_responses.append((midas,entity,text))\n",
    "    \n",
    "    # calc stats\n",
    "    zero_midas_counter.update([midas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "728cf855-35e4-449f-9d3c-ad645d754110",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/convert_responses_zero.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(zero_responses, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0c45d-36f5-4b6e-9612-76a2e9130a78",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convert stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "1a75e2ad-ae08-4353-a187-32bc30a2195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('opinion', 4096),\n",
       " ('statement', 4029),\n",
       " ('yes_no_question', 868),\n",
       " ('pos_answer', 214),\n",
       " ('comment', 177),\n",
       " ('open_question_factual', 171),\n",
       " ('command', 170),\n",
       " ('open_question_opinion', 168),\n",
       " ('neg_answer', 66),\n",
       " ('complaint', 16),\n",
       " ('dev_command', 10),\n",
       " ('appreciation', 5),\n",
       " ('other_answers', 1)]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midas_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "4d1295bd-3b53-4ba5-87e0-bc9abf452b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('opinion', 2822),\n",
       " ('pos_answer', 2134),\n",
       " ('statement', 2091),\n",
       " ('comment', 860),\n",
       " ('neg_answer', 524),\n",
       " ('appreciation', 351),\n",
       " ('complaint', 338),\n",
       " ('open_question_factual', 220),\n",
       " ('yes_no_question', 210),\n",
       " ('other_answers', 179),\n",
       " ('command', 137),\n",
       " ('open_question_opinion', 102),\n",
       " ('dev_command', 23)]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_midas_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "01c1c0cb-6cd3-4823-b292-fd97008056e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person', 2277),\n",
       " ('videoname', 1477),\n",
       " ('location', 1136),\n",
       " ('organization', 813),\n",
       " ('device', 660),\n",
       " ('duration', 629),\n",
       " ('genre', 542),\n",
       " ('sport', 508),\n",
       " ('number', 465),\n",
       " ('sportteam', 394),\n",
       " ('softwareapplication', 310),\n",
       " ('vehicle', 177),\n",
       " ('event', 148),\n",
       " ('position', 133),\n",
       " ('date', 130),\n",
       " ('year', 85),\n",
       " ('gamename', 57),\n",
       " ('party', 37),\n",
       " ('bookname', 9),\n",
       " ('songname', 4)]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "1e0f829c-404b-4eb1-8f5e-5ea2614ec05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('opinion person', 1116),\n",
       " ('statement person', 730),\n",
       " ('opinion videoname', 694),\n",
       " ('statement location', 545),\n",
       " ('statement videoname', 518),\n",
       " ('statement duration', 493),\n",
       " ('opinion location', 394),\n",
       " ('opinion organization', 374),\n",
       " ('statement number', 366),\n",
       " ('opinion genre', 314),\n",
       " ('statement organization', 289),\n",
       " ('statement device', 271),\n",
       " ('opinion sport', 251),\n",
       " ('opinion sportteam', 221),\n",
       " ('opinion device', 218),\n",
       " ('yes_no_question person', 215),\n",
       " ('yes_no_question videoname', 169),\n",
       " ('statement softwareapplication', 148),\n",
       " ('statement sport', 135),\n",
       " ('statement sportteam', 133),\n",
       " ('statement genre', 105),\n",
       " ('opinion softwareapplication', 99),\n",
       " ('statement vehicle', 88),\n",
       " ('yes_no_question device', 83),\n",
       " ('yes_no_question location', 81),\n",
       " ('opinion duration', 76),\n",
       " ('statement date', 71),\n",
       " ('opinion position', 65),\n",
       " ('opinion event', 60),\n",
       " ('yes_no_question sport', 59),\n",
       " ('yes_no_question organization', 57),\n",
       " ('opinion number', 53),\n",
       " ('comment person', 53),\n",
       " ('yes_no_question genre', 53),\n",
       " ('statement event', 50),\n",
       " ('open_question_opinion person', 48),\n",
       " ('opinion vehicle', 47),\n",
       " ('opinion year', 40),\n",
       " ('open_question_factual person', 37),\n",
       " ('pos_answer genre', 37),\n",
       " ('yes_no_question softwareapplication', 37),\n",
       " ('pos_answer person', 34),\n",
       " ('open_question_factual location', 33),\n",
       " ('statement year', 30),\n",
       " ('command person', 29),\n",
       " ('opinion gamename', 29),\n",
       " ('comment videoname', 26),\n",
       " ('pos_answer videoname', 26),\n",
       " ('open_question_opinion location', 25),\n",
       " ('opinion date', 24),\n",
       " ('comment location', 21),\n",
       " ('statement gamename', 21),\n",
       " ('comment organization', 21),\n",
       " ('pos_answer sport', 21),\n",
       " ('pos_answer organization', 20),\n",
       " ('yes_no_question duration', 20),\n",
       " ('statement position', 20),\n",
       " ('yes_no_question event', 19),\n",
       " ('opinion party', 18),\n",
       " ('command position', 18),\n",
       " ('open_question_factual device', 18),\n",
       " ('command organization', 18),\n",
       " ('command device', 18),\n",
       " ('yes_no_question vehicle', 18),\n",
       " ('open_question_factual organization', 17),\n",
       " ('yes_no_question date', 16),\n",
       " ('yes_no_question sportteam', 15),\n",
       " ('command location', 15),\n",
       " ('pos_answer number', 14),\n",
       " ('open_question_opinion device', 14),\n",
       " ('open_question_opinion organization', 13),\n",
       " ('neg_answer location', 13),\n",
       " ('command videoname', 13),\n",
       " ('open_question_factual vehicle', 13),\n",
       " ('neg_answer person', 13),\n",
       " ('comment device', 12),\n",
       " ('pos_answer device', 12),\n",
       " ('command sport', 12),\n",
       " ('pos_answer softwareapplication', 11),\n",
       " ('open_question_factual duration', 11),\n",
       " ('command genre', 10),\n",
       " ('open_question_opinion sport', 10),\n",
       " ('command number', 9),\n",
       " ('statement party', 9),\n",
       " ('open_question_opinion position', 9),\n",
       " ('open_question_opinion videoname', 9),\n",
       " ('open_question_factual videoname', 9),\n",
       " ('open_question_opinion genre', 9),\n",
       " ('pos_answer year', 8),\n",
       " ('neg_answer sport', 8),\n",
       " ('command duration', 8),\n",
       " ('comment position', 8),\n",
       " ('yes_no_question gamename', 7),\n",
       " ('yes_no_question position', 7),\n",
       " ('comment sportteam', 7),\n",
       " ('comment sport', 7),\n",
       " ('open_question_factual event', 7),\n",
       " ('dev_command device', 7),\n",
       " ('open_question_opinion number', 7),\n",
       " ('pos_answer duration', 6),\n",
       " ('comment genre', 6),\n",
       " ('neg_answer videoname', 6),\n",
       " ('pos_answer location', 6),\n",
       " ('neg_answer device', 6),\n",
       " ('open_question_opinion sportteam', 6),\n",
       " ('pos_answer sportteam', 5),\n",
       " ('pos_answer date', 5),\n",
       " ('open_question_opinion date', 5),\n",
       " ('pos_answer party', 5),\n",
       " ('open_question_opinion duration', 5),\n",
       " ('yes_no_question number', 5),\n",
       " ('neg_answer duration', 5),\n",
       " ('open_question_factual number', 5),\n",
       " ('command event', 5),\n",
       " ('statement bookname', 5),\n",
       " ('open_question_factual position', 5),\n",
       " ('open_question_factual date', 4),\n",
       " ('command softwareapplication', 4),\n",
       " ('comment number', 4),\n",
       " ('command date', 4),\n",
       " ('neg_answer softwareapplication', 4),\n",
       " ('open_question_opinion event', 4),\n",
       " ('open_question_factual sport', 4),\n",
       " ('complaint videoname', 3),\n",
       " ('opinion bookname', 3),\n",
       " ('yes_no_question party', 3),\n",
       " ('comment softwareapplication', 3),\n",
       " ('pos_answer vehicle', 3),\n",
       " ('comment vehicle', 3),\n",
       " ('complaint organization', 3),\n",
       " ('appreciation videoname', 3),\n",
       " ('complaint location', 3),\n",
       " ('open_question_factual year', 3),\n",
       " ('open_question_factual genre', 3),\n",
       " ('command sportteam', 3),\n",
       " ('neg_answer genre', 3),\n",
       " ('comment event', 2),\n",
       " ('complaint duration', 2),\n",
       " ('statement songname', 2),\n",
       " ('command vehicle', 2),\n",
       " ('neg_answer sportteam', 2),\n",
       " ('yes_no_question year', 2),\n",
       " ('complaint softwareapplication', 2),\n",
       " ('neg_answer number', 2),\n",
       " ('open_question_opinion vehicle', 2),\n",
       " ('comment duration', 2),\n",
       " ('complaint genre', 1),\n",
       " ('open_question_opinion softwareapplication', 1),\n",
       " ('pos_answer event', 1),\n",
       " ('open_question_factual sportteam', 1),\n",
       " ('command songname', 1),\n",
       " ('appreciation person', 1),\n",
       " ('complaint person', 1),\n",
       " ('yes_no_question songname', 1),\n",
       " ('neg_answer position', 1),\n",
       " ('open_question_opinion party', 1),\n",
       " ('comment year', 1),\n",
       " ('dev_command videoname', 1),\n",
       " ('yes_no_question bookname', 1),\n",
       " ('neg_answer date', 1),\n",
       " ('comment party', 1),\n",
       " ('dev_command genre', 1),\n",
       " ('neg_answer vehicle', 1),\n",
       " ('appreciation device', 1),\n",
       " ('neg_answer organization', 1),\n",
       " ('command year', 1),\n",
       " ('other_answers sportteam', 1),\n",
       " ('open_question_factual softwareapplication', 1),\n",
       " ('dev_command duration', 1),\n",
       " ('complaint sport', 1)]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midas_entity_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea201d4-572a-4668-b21b-289ef76907e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocess datasets for train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa0351a0-7b25-4b8d-97e8-637d8d62b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179286, 39089, 5206)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_train_eval =  SequencePreprocessor(seq_validator=None)\n",
    "train_dataset = for_train_eval.transform(train_seqs)\n",
    "val_dataset = for_train_eval.transform(val_seqs)\n",
    "test_dataset = for_train_eval.transform(test_seqs)\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c142599-53c7-4270-8fe1-81c085db2195",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### save context for convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1814e046-87ec-4322-973e-881e9d81edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_contexts = list()\n",
    "test_contexts = list()\n",
    "\n",
    "for seq in val_dataset:\n",
    "    val_contexts.append(seq['previous_text'])\n",
    "    \n",
    "for seq in test_dataset:\n",
    "    test_contexts.append(seq['previous_text'])\n",
    "    \n",
    "with open('data/annotated/val_contexts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_contexts, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "with open('data/annotated/test_contexts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_contexts, f, ensure_ascii=False, indent=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "625a6d7b-fbb1-4154-8ad8-4dbae1ff78c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'previous_text': [['Hi!', ' Have you every played golf?'],\n",
       "  ['Hey I actually have played golf.',\n",
       "   \"Even though I'm horrible haha what about you?\"],\n",
       "  ['I have made noble attempts.',\n",
       "   'but I am horrible.',\n",
       "   ' Definitely not going to quit my day job.']],\n",
       " 'previous_midas': [['statement', 'yes_no_question'],\n",
       "  ['statement', 'open_question_opinion'],\n",
       "  ['statement', 'opinion', 'opinion']],\n",
       " 'midas_vectors': [[[0.008263594470918179,\n",
       "    0.01513385958969593,\n",
       "    0.012243056669831276,\n",
       "    0.013781467452645302,\n",
       "    0.016995860263705254,\n",
       "    0.0018954542465507984,\n",
       "    0.007130841724574566,\n",
       "    0.00389679754152894,\n",
       "    0.055932093411684036,\n",
       "    0.004719457123428583,\n",
       "    0.004707093816250563,\n",
       "    0.8473060727119446,\n",
       "    0.007994434796273708],\n",
       "   [0.0053682513535022736,\n",
       "    0.08350381255149841,\n",
       "    0.005704871378839016,\n",
       "    0.02419126406311989,\n",
       "    0.021962281316518784,\n",
       "    0.0038323821499943733,\n",
       "    0.02248297818005085,\n",
       "    0.011466488242149353,\n",
       "    0.0012161567574366927,\n",
       "    0.0028420155867934227,\n",
       "    0.00548276212066412,\n",
       "    0.0057818703353405,\n",
       "    0.8061648607254028]],\n",
       "  [[0.0046763950958848,\n",
       "    0.017610564827919006,\n",
       "    0.005370092112571001,\n",
       "    0.006485395599156618,\n",
       "    0.004874098114669323,\n",
       "    0.005754752084612846,\n",
       "    0.0023053698241710663,\n",
       "    0.0018256386974826455,\n",
       "    0.01688682660460472,\n",
       "    0.0026423821691423655,\n",
       "    0.43541038036346436,\n",
       "    0.48983433842658997,\n",
       "    0.006323758978396654],\n",
       "   [0.012717173434793949,\n",
       "    0.005554381292313337,\n",
       "    0.0046592275612056255,\n",
       "    0.0035295814741402864,\n",
       "    0.006701064296066761,\n",
       "    0.0017675000708550215,\n",
       "    0.030794894322752953,\n",
       "    0.8908878564834595,\n",
       "    0.016421256586909294,\n",
       "    0.004280492663383484,\n",
       "    0.0023977644741535187,\n",
       "    0.0024137995205819607,\n",
       "    0.01787504181265831]],\n",
       "  [[0.001011829823255539,\n",
       "    0.002804309129714966,\n",
       "    0.003142371540889144,\n",
       "    0.0017116317758336663,\n",
       "    0.002004690235480666,\n",
       "    0.0005941958515904844,\n",
       "    0.0009058901923708618,\n",
       "    0.0006202768418006599,\n",
       "    0.047946471720933914,\n",
       "    0.001120253698900342,\n",
       "    0.0012502202298492193,\n",
       "    0.9358071684837341,\n",
       "    0.0010807544458657503],\n",
       "   [0.0010154879419133067,\n",
       "    0.0017161080613732338,\n",
       "    0.0038511077873408794,\n",
       "    0.0011673913104459643,\n",
       "    0.0008156256517395377,\n",
       "    0.000957709620706737,\n",
       "    0.001140137668699026,\n",
       "    0.000783205556217581,\n",
       "    0.9503637552261353,\n",
       "    0.0010892912978306413,\n",
       "    0.0016611508326604962,\n",
       "    0.034820299595594406,\n",
       "    0.000618654943536967],\n",
       "   [0.0016846263315528631,\n",
       "    0.009879065677523613,\n",
       "    0.0044670505449175835,\n",
       "    0.003698512678965926,\n",
       "    0.003597152652218938,\n",
       "    0.005001523531973362,\n",
       "    0.005005447659641504,\n",
       "    0.002532775979489088,\n",
       "    0.9040650725364685,\n",
       "    0.001750329858623445,\n",
       "    0.00366371963173151,\n",
       "    0.05246228724718094,\n",
       "    0.0021923843305557966]]],\n",
       " 'previous_entities': [[[],\n",
       "   [{'label': 'sport', 'offsets': [23, 27], 'text': 'golf'}]],\n",
       "  [[{'label': 'sport', 'offsets': [27, 31], 'text': 'golf'}], []],\n",
       "  [[{'label': 'misc', 'offsets': [12, 26], 'text': 'noble attempts'}],\n",
       "   [{'label': 'misc', 'offsets': [9, 17], 'text': 'horrible'}],\n",
       "   [{'label': 'misc', 'offsets': [30, 36], 'text': 'my day'}]]],\n",
       " 'predict': {'text': \"nope and what's weird with SPORT is that it doesn't use a standardized playing area.\",\n",
       "  'midas': 'opinion',\n",
       "  'entities': [{'label': 'sport', 'offsets': [27, 31], 'text': 'golf'}]}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5034f7-509d-4328-b10b-ddd700afa942",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4ddce20-becf-489c-aa50-a7f5c901cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ead37-17bd-4a95-9718-4b3fb3e8f6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "os.environ['TFHUB_CACHE_DIR'] = './models/tf_cache'\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "encoder = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08cb2c68-19b8-4436-96cc-fb1b287c0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_cnt = Counter()\n",
    "\n",
    "for sample in train_dataset:\n",
    "    for ut in sample['previous_entities']:\n",
    "        entities_cnt.update([ent['label'] for sent in ut for ent in sent if sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c478ea62-b774-40ed-922e-fed9614fdd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity2ID = {label[0]: i for i, label in enumerate(entities_cnt.most_common())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb2467a3-1ac2-432c-8b03-f243eac52659",
   "metadata": {},
   "outputs": [],
   "source": [
    "Midas2ID = {\n",
    "    \"appreciation\": 0, \"command\": 1, \"comment\": 2,\"complaint\": 3,\n",
    "    \"dev_command\": 4, \"neg_answer\": 5, \"open_question_factual\": 6,\n",
    "    \"open_question_opinion\": 7, \"opinion\": 8, \"other_answers\": 9,\n",
    "    \"pos_answer\": 10, \"statement\": 11, \"yes_no_question\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f75313c-915c-4bd6-a5f4-5d9b220b0934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'misc': 0,\n",
       " 'person': 1,\n",
       " 'location': 2,\n",
       " 'videoname': 3,\n",
       " 'organization': 4,\n",
       " 'device': 5,\n",
       " 'sport': 6,\n",
       " 'duration': 7,\n",
       " 'number': 8,\n",
       " 'genre': 9,\n",
       " 'sportteam': 10,\n",
       " 'position': 11,\n",
       " 'event': 12,\n",
       " 'softwareapplication': 13,\n",
       " 'anaphor': 14,\n",
       " 'vehicle': 15,\n",
       " 'party': 16,\n",
       " 'year': 17,\n",
       " 'date': 18,\n",
       " 'gamename': 19,\n",
       " 'songname': 20,\n",
       " 'bookname': 21}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entity2ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0f45887c-9556-4f53-a9d3-cf4541fa83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EntityLabelEncoder = MultiLabelBinarizer()\n",
    "EntityLabelEncoder.classes = [label for label in Entity2ID if label not in ['misc', 'anaphor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6abe0694-e0f6-43e4-9d16-d2a56b6b9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleVectorizer:\n",
    "    \n",
    "    def __init__(\n",
    "        self, text_vectorizer, entity2id:dict, midas2id:dict,\n",
    "        context_len:int=3, embed_dim:int = 512):\n",
    "        \n",
    "        self.vectorizer = text_vectorizer\n",
    "        self.entity2id = entity2id\n",
    "        self.midas2id = midas2id\n",
    "        self.context_len = context_len\n",
    "        # 512 + 13 + 22 = 547\n",
    "        self.utterance_vec_size = embed_dim + len(midas2id) + len(entity2id)\n",
    "        # 3 * 547 = 1641\n",
    "        self.vector_size = self.context_len * self.utterance_vec_size\n",
    "        \n",
    "        \n",
    "    def context_vector(\n",
    "        self, context: list, midas_vectors: list, entities: list) -> tuple:\n",
    "        \"\"\"\n",
    "        vectorizes the previous context by concatenating text embeddings,\n",
    "        midas probas and one-hot encoded entities \"\"\"\n",
    "        embedding = self.__embed(context)\n",
    "        midas = self.__norm_midas(midas_vectors)\n",
    "        entities = self.__oh_encode(entities)\n",
    "        return self.__get_context_vec(embedding, midas, entities)\n",
    "        \n",
    "        \n",
    "    def __embed(self, utterances: list) -> np.ndarray:\n",
    "        \"\"\" \n",
    "        vectorizes a list of N previous utterances using a provided encoder\n",
    "        input: List[str]\n",
    "        output: numpy array (len(utterance), embed_dim)\n",
    "        \"\"\"\n",
    "        return self.vectorizer([\" \".join(ut) for ut in utterances]).numpy()\n",
    "    \n",
    "    \n",
    "    def __norm_midas(self, midas_vectors: list) -> np.ndarray:\n",
    "        \"\"\" \n",
    "        takes midas vectors of all sentences in the utterance\n",
    "        and returns a vector with max values per midas label\n",
    "        \"\"\"\n",
    "        vecs = np.zeros((len(midas_vectors), 13))\n",
    "        \n",
    "        for i, vec in enumerate(midas_vectors):\n",
    "            # get max probability per each midas labels\n",
    "            vecs[i] = np.max(np.array(vec), axis=0)\n",
    "\n",
    "        # return normalized\n",
    "        return vecs\n",
    "    \n",
    "    \n",
    "    def __oh_encode(self, entities) -> np.ndarray:\n",
    "        \"\"\" one-hot encoding of entities per each sample \"\"\"\n",
    "        entities = [[ent['label'] for sent in ut for ent in sent] for ut in entities]\n",
    "        ohe_vec = np.zeros((len(entities), len(self.entity2id)))\n",
    "        \n",
    "        for i, ut in enumerate(entities):\n",
    "            for ent in set(ut):\n",
    "                ent_id = self.entity2id.get(ent, None)\n",
    "                if not ent_id:\n",
    "                    continue\n",
    "                ohe_vec[i][ent_id] = 1\n",
    "                \n",
    "        return ohe_vec\n",
    "    \n",
    "    \n",
    "    def __get_context_vec(self, embedding: np.ndarray,\n",
    "                      midas_vec: np.ndarray, \n",
    "                      ohe_vec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" \n",
    "        concatenates text embeddings with midas vectors \n",
    "        and one-hot encoded entities\n",
    "        \n",
    "        The output vector will be (n_utterances, self.vector_dim)\n",
    "        Vector dim comes from:\n",
    "        1. [embedding of utterance(i-2)]\n",
    "        2. [midas proba distribution utterance(i-2)]\n",
    "        3. [entity type one-hot utterance(i-2)]\n",
    "        4. [embedding (i-1)]\n",
    "        5. [midas (i-1)][entity (i-1)]\n",
    "        6. [embedding (i)] \n",
    "        7. [midas (i)]\n",
    "        8. [entity (i)]\n",
    "        \"\"\"\n",
    "        assert embedding.shape[0] == midas_vec.shape[0] == ohe_vec.shape[0]\n",
    "        vecs = np.zeros((self.context_len, self.utterance_vec_size))\n",
    "\n",
    "        vecs[:,:embedding.shape[1]] = embedding\n",
    "        vecs[:,embedding.shape[1]:embedding.shape[1]+midas_vec.shape[1]] = midas_vec\n",
    "        vecs[:,embedding.shape[1]+midas_vec.shape[1]:] = ohe_vec\n",
    "        \n",
    "        vecs = vecs.reshape(-1)\n",
    "        \n",
    "        assert vecs.shape[0] == self.vector_size\n",
    "\n",
    "        # returned context vector (1, n_ut * utterance_dim)\n",
    "        return vecs.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83bc69b4-e3cf-4715-8fed-669ace9cc9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = SampleVectorizer(\n",
    "    text_vectorizer=encoder, # USE\n",
    "    entity2id=Entity2ID,\n",
    "    midas2id=Midas2ID,\n",
    "    context_len=3,\n",
    "    embed_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "57f42906-6a1b-4fc7-b9c1-502da35bdd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03675972, -0.03856859,  0.0011045 ,  0.06951697,  0.0160016 ,\n",
       "        0.06346866, -0.09389518,  0.02070764,  0.04127781, -0.01470397,\n",
       "       -0.04645021,  0.00437406,  0.00826359,  0.08350381,  0.01224306,\n",
       "        0.02419126,  0.02196228,  0.00383238,  0.02248298,  0.01146649,\n",
       "        0.05593209,  0.00471946,  0.00548276,  0.84730607,  0.80616486,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01185059, -0.01601361, -0.0223879 ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.context_vector(\n",
    "    train_dataset[0]['previous_text'],\n",
    "    train_dataset[0]['midas_vectors'],\n",
    "    train_dataset[0]['previous_entities']\n",
    ")[500:550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "31df4993-51bd-4efd-b858-b6e84814ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class Dataset(Sequence):\n",
    "    \n",
    "    \"\"\" customized Dataset class from torch \"\"\"\n",
    "    \n",
    "    def __init__(self, data: list, vectorizer, batch_size: int = 32, shuffle: bool = False):\n",
    "        self.data = data\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        self.vectorizer = vectorizer\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle=shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch\n",
    "        A common practice is to set this value to [num_samples / batch sizeâŒ‹\n",
    "        so that the model sees the training samples at most once per epoch.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch\n",
    "        Shuffling the order so that batches between epochs do not look alike.\n",
    "        It can make a model more robust.\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\" get batch_id and return its vectorized representation \"\"\"\n",
    "        indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch = [self.data[index] for index in indexes]\n",
    "        \n",
    "        x_batch = np.zeros([len(batch), self.vectorizer.vector_size])\n",
    "        y_batch = list()\n",
    "        \n",
    "        for i, sample in enumerate(batch):\n",
    "            x_batch[i, :] = self.vectorizer.context_vector(\n",
    "                sample['previous_text'],\n",
    "                sample['midas_vectors'], sample['previous_entities'])\n",
    "            \n",
    "            midas_label = sample['predict']['midas']\n",
    "            \n",
    "            entity_labels = [ent['label'] for ent in sample['predict']['entities'] if ent]\n",
    "            y_batch.append([midas_label, entity_labels])\n",
    "        \n",
    "        \n",
    "        return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7a8e43e6-2e12-409b-a1a5-4d5a218ad3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataset(\n",
    "    data=train_dataset,\n",
    "    vectorizer=vectorizer,\n",
    "    batch_size=32,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e42f92f7-234b-4b98-8593-a855355b6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1641)\n"
     ]
    }
   ],
   "source": [
    "x, y = train_loader[17]\n",
    "    \n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "484d5122-60a5-4640-a5c4-0e441a593831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['opinion', ['organization']],\n",
       " ['statement', ['location', 'device']],\n",
       " ['opinion', []],\n",
       " ['pos_answer', []],\n",
       " ['statement', []],\n",
       " ['opinion', ['person', 'device', 'device', 'person']],\n",
       " ['statement', ['genre']],\n",
       " ['statement', ['videoname']],\n",
       " ['opinion', ['person']],\n",
       " ['statement', []],\n",
       " ['opinion', ['location']],\n",
       " ['statement', ['videoname']],\n",
       " ['opinion', []],\n",
       " ['opinion', []],\n",
       " ['statement', []],\n",
       " ['opinion', ['person']],\n",
       " ['statement', []],\n",
       " ['pos_answer', []],\n",
       " ['comment', []],\n",
       " ['statement', []],\n",
       " ['opinion', ['vehicle', 'vehicle']],\n",
       " ['opinion', []],\n",
       " ['statement', ['location']],\n",
       " ['comment', []],\n",
       " ['statement', []],\n",
       " ['comment', []],\n",
       " ['statement', []],\n",
       " ['opinion', []],\n",
       " ['opinion', []],\n",
       " ['opinion', []],\n",
       " ['statement', []],\n",
       " ['opinion', []]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825f71c-b789-4e23-b6e8-825ba6a82ec6",
   "metadata": {},
   "source": [
    "#### split midas and entity labels to encode them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ff2c470f-ad0c-430e-bdcd-698591b3ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split midas and entites\n",
    "y_midas = list()\n",
    "y_entity = list()\n",
    "\n",
    "for label in y:\n",
    "    # encode midas labels\n",
    "    y_midas.append(Midas2ID[label[0]])\n",
    "    y_entity.append(label[1])\n",
    "    \n",
    "# encode entity labels\n",
    "y_entity = EntityLabelEncoder.fit_transform(y_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "703b97ed-99c1-487a-91f5-9eea8495e3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ca8fe1d4-40d9-4aae-b0da-0b27bc795591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check OOV labels\n",
    "EntityLabelEncoder.fit_transform([['asd', 'person'], ['person', 'vehicle']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb12b1-4dd7-4d29-892b-acd6a67e145d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### train on a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b7aca8c1-2f75-4342-9f2d-5ed528d956d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataset(\n",
    "    data=train_dataset,\n",
    "    vectorizer=vectorizer,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=False)\n",
    "\n",
    "val_loader = Dataset(\n",
    "    data=val_dataset,\n",
    "    vectorizer=vectorizer,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "14bdb6c4-1715-47c5-b1c9-0e5ad202f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_train, y_train in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2c2985be-a824-4906-8dc5-d6a9c0338c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split midas and entites\n",
    "y_midas_train = list()\n",
    "y_entity_train = list()\n",
    "\n",
    "for label in y_train:\n",
    "    # encode midas labels\n",
    "    y_midas_train.append(Midas2ID[label[0]])\n",
    "    y_entity_train.append(label[1])\n",
    "    \n",
    "# encode entity labels\n",
    "y_entity_train = EntityLabelEncoder.fit_transform(y_entity_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "334c1bc4-ea4a-4a9a-b9be-c67c0a9d526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/vectorized_train.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "    np.save(f, y_midas_train)\n",
    "    np.save(f, y_entity_train)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "600322ad-5097-4569-90c4-05b1379dce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_val, y_val in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1a7caf72-7bea-4082-8362-826a7d4a6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split midas and entites\n",
    "y_midas_val = list()\n",
    "y_entity_val = list()\n",
    "\n",
    "for label in y_val:\n",
    "    # encode midas labels\n",
    "    y_midas_val.append(Midas2ID[label[0]])\n",
    "    y_entity_val.append(label[1])\n",
    "    \n",
    "# encode entity labels\n",
    "y_entity_val = EntityLabelEncoder.fit_transform(y_entity_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "24ec3446-783f-4a72-baf3-9f3f86bd1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/vectorized_val.npy', 'wb') as f:\n",
    "    np.save(f, X_val)\n",
    "    np.save(f, y_midas_val)\n",
    "    np.save(f, y_entity_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2b33ea1c-559a-4e8f-945f-9f9ccf41af8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((179286, 1641), 179286, 179286)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, len(y_midas_train), len(y_entity_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fe3df-4c79-4267-9d7c-6ce2e3788ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3c01d-1030-478b-b591-8475f41b8c9c",
   "metadata": {},
   "source": [
    "with open('data/annotated/vectorized_train.npy', 'rb') as f:\n",
    "    X_train = np.load(f)\n",
    "    y_midas_train = np.load(f)\n",
    "    y_entity_train = np.load(f)\n",
    "\n",
    "with open('data/annotated/vectorized_val.npy', 'rb') as f:\n",
    "    X_val = np.load(f)\n",
    "    y_midas_val = np.load(f)\n",
    "    y_entity_val = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "730bddca-59ca-4211-b4e7-80a2733e2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de7229-8806-46f0-aee3-80dbb1600c6d",
   "metadata": {},
   "source": [
    "### Midas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8a726b72-5ad4-4a67-b799-5cfe74ddea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_midas = RandomForestClassifier(max_depth=20, max_samples=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0eae1ced-f175-429a-b309-e4497ea11ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=20, max_samples=0.4, random_state=42)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_midas.fit(X_train, y_midas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6c7baa0a-9f23-4627-9657-fe0743efc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "midas_preds = rfc_midas.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "09714d96-5693-465c-bd4a-b8eb5daf2fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({11: 13016,\n",
       "         8: 25526,\n",
       "         2: 88,\n",
       "         10: 252,\n",
       "         12: 64,\n",
       "         5: 32,\n",
       "         1: 28,\n",
       "         7: 15,\n",
       "         6: 42,\n",
       "         3: 19,\n",
       "         0: 5,\n",
       "         4: 1,\n",
       "         9: 1})"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(midas_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8e12a953-976f-476c-8f67-1d9e51214eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40740361738596537"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_midas_val, midas_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "38bc70e6-be78-4af6-8a16-6e7a3a75e9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3172102899963896"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_midas_val, midas_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c51cf4fe-c3e8-467e-8781-a6b85e76a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open('models/rfcME_midas_depth20_maxsample04.pickle', 'wb') as f:\n",
    "    pickle.dump(rfc_midas, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c625bc-3843-4391-bc69-216e9238454f",
   "metadata": {},
   "source": [
    "### Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e1176915-d357-44c9-908f-2afd0d640526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "c5fbe78e-49e4-4f40-86b9-4fe1b45a6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_entity = RandomForestClassifier(max_depth=20, max_samples=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c6ee8b3c-1b1b-493b-86af-4977281709f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_clf = OneVsRestClassifier(rfc_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "12b5a14c-8504-41e9-a566-17fd2a4014a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier(max_depth=20,\n",
       "                                                     max_samples=0.4,\n",
       "                                                     random_state=42))"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_clf.fit(X_train, y_entity_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fc1da9a0-135d-47ae-b40b-c7ff6098efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_preds = entity_clf.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "d0139738-eb2a-4dd7-9aae-d19466b5b795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 13547),\n",
       " (1, 8173),\n",
       " (2, 3803),\n",
       " (3, 2909),\n",
       " (4, 2213),\n",
       " (5, 2003),\n",
       " (6, 1447),\n",
       " (7, 1237),\n",
       " (8, 912),\n",
       " (9, 830),\n",
       " (13, 653),\n",
       " (12, 485),\n",
       " (10, 357),\n",
       " (16, 196),\n",
       " (11, 121),\n",
       " (14, 108),\n",
       " (15, 50),\n",
       " (17, 32),\n",
       " (19, 8),\n",
       " (18, 5)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cnt = Counter()\n",
    "\n",
    "for ent in entity_preds:\n",
    "    pr = np.argmax(ent)\n",
    "    preds_cnt.update([pr])\n",
    "    \n",
    "preds_cnt.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "40f595ff-eb85-44a5-b771-22c1b933cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_preds = entity_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "132f2f20-cad5-413a-8ce6-7fb704ab1a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8460436439919159"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_entity_val, entity_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b82afd62-acab-43ae-a05d-181954ca5074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for ent in entity_preds:\n",
    "    if sum(ent) > 0:\n",
    "        print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "89697a2c-5fb2-47fa-ad7d-beebb216ab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00149589, 0.        , 0.        , 0.        , 0.00410678,\n",
       "       0.01058201, 0.0344086 , 0.00438596, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_entity_val, entity_preds, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "620fdf12-5341-46d8-ba21-76d1f2ebb237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/entity_clf_depth20_maxsplit_04.joblib']"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(entity_clf, 'models/entity_clf_depth20_maxsplit_04.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b0bbf4e3-322c-4e62-bfd9-35bcb1293933",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_clf = load('models/entity_clf_depth20_maxsplit_04.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "546c7ee0-76f0-4074-a415-4b2b1055e357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 13547),\n",
       " (1, 8173),\n",
       " (2, 3803),\n",
       " (3, 2909),\n",
       " (4, 2213),\n",
       " (5, 2003),\n",
       " (6, 1447),\n",
       " (7, 1237),\n",
       " (8, 912),\n",
       " (9, 830),\n",
       " (13, 653),\n",
       " (12, 485),\n",
       " (10, 357),\n",
       " (16, 196),\n",
       " (11, 121),\n",
       " (14, 108),\n",
       " (15, 50),\n",
       " (17, 32),\n",
       " (19, 8),\n",
       " (18, 5)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_cnt = Counter()\n",
    "\n",
    "for ent in loaded_clf.predict_proba(X_val):\n",
    "    pr = np.argmax(ent)\n",
    "    preds_cnt.update([pr])\n",
    "    \n",
    "preds_cnt.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8a572-53ba-4d76-9ac7-9d4ed95a4797",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9fad6-297a-4322-be64-19470665b6c3",
   "metadata": {},
   "source": [
    "possible responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c374c-f9f3-46f0-a174-3f1232918309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "0c7cb076-0140-45ad-a091-1a3ea3e4fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/annotated/convert_responses.json', 'r', encoding='utf8') as f:\n",
    "    responses = json.load(f)\n",
    "    \n",
    "with open('data/annotated/convert_responses_zero.json', 'r', encoding='utf8') as f:\n",
    "    responses_zero = json.load(f)\n",
    "\n",
    "responses = pd.DataFrame(responses + responses_zero, columns=['midas', 'entity', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "23d3b4e0-b132-42c6-ada9-f75284a282c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midas</th>\n",
       "      <th>entity</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statement</td>\n",
       "      <td>sport</td>\n",
       "      <td>i didn't even realize that he played any SPORT.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opinion</td>\n",
       "      <td>person</td>\n",
       "      <td>i head it was because PERSON was so dominant a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statement</td>\n",
       "      <td>organization</td>\n",
       "      <td>usually though ORGANIZATION.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statement</td>\n",
       "      <td>location</td>\n",
       "      <td>i don't think anything is happening in LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>statement</td>\n",
       "      <td>softwareapplication</td>\n",
       "      <td>not unless SOFTWAREAPPLICATION publishes their...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       midas               entity  \\\n",
       "0  statement                sport   \n",
       "1    opinion               person   \n",
       "2  statement         organization   \n",
       "3  statement             location   \n",
       "4  statement  softwareapplication   \n",
       "\n",
       "                                                text  \n",
       "0    i didn't even realize that he played any SPORT.  \n",
       "1  i head it was because PERSON was so dominant a...  \n",
       "2                       usually though ORGANIZATION.  \n",
       "3  i don't think anything is happening in LOCATIO...  \n",
       "4  not unless SOFTWAREAPPLICATION publishes their...  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d9ae1ab8-72f1-463e-a449-d9c7caa89422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([10006, 10010, 10013, 10016, 10018, 10022, 10027, 10028, 10032,\n",
       "            10033,\n",
       "            ...\n",
       "            19945, 19951, 19952, 19957, 19958, 19963, 19970, 19975, 19979,\n",
       "            19980],\n",
       "           dtype='int64', length=2822)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.index[(responses.midas == 'opinion') & (responses.entity.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "5beba503-df81-434b-b0bc-bbf60fff911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/convert_vecs/resp_encoding.npy', 'rb') as f: \n",
    "    responses_vecs = np.load(f)\n",
    "    \n",
    "with open('data/convert_vecs/resp_encoding_zero.npy', 'rb') as f: \n",
    "    responses_vecs_zero = np.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "aed9d3b7-c83f-4e6d-abd8-fc6bc62647cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/convert_vecs/test_context_encoding.npy', 'rb') as f: \n",
    "    context_vecs = list()\n",
    "    for i in range(len(test_seqs)):\n",
    "        context_vecs.append(np.load(f))\n",
    "        \n",
    "context_vecs = np.vstack(context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "a64682b1-ba24-4bb9-b3b1-64e80beb4e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19982, 512), (9991, 512), (5206, 512))"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_vecs.shape, responses_vecs_zero.shape, context_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "6b65db2e-90db-4dcb-a9a4-032778d38117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19982, 512)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "913b47df-b1b1-4547-a0e9-ee56cba0dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_vecs = np.vstack((responses_vecs, responses_vecs_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "65d2fd4a-02d4-498d-ba32-6be7d9b0e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = SampleVectorizer(\n",
    "    text_vectorizer=encoder, # USE\n",
    "    entity2id=Entity2ID,\n",
    "    midas2id=Midas2ID,\n",
    "    context_len=3,\n",
    "    embed_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "4719bbc3-49ae-4c7b-a680-8122f84f99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityChecker:\n",
    "    \n",
    "    def __init__(\n",
    "        self, entity2id: list, stoplist: list = ['misc', 'anaphor']):\n",
    "        self.entity2id = entity2id\n",
    "        self.stoplist = stoplist\n",
    "        \n",
    "    def get_context_entities(self, sample: dict) -> list:\n",
    "        \"\"\" \n",
    "        returns all the entities from the context\n",
    "        except those from the stoplist\n",
    "        \n",
    "        output:\n",
    "        entities: List[Tuple]\n",
    "        \"\"\"\n",
    "        entities = [self.__get_entities(ut) for ut in sample['previous_entities']]\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def __get_entities(self, ut: list) -> list:\n",
    "        \"\"\" extract entities from a single utterance \"\"\"\n",
    "        ents = [\n",
    "            (ent['label'], ent['text'], self.entity2id[ent['label']]) \n",
    "            for sent in ut for ent in sent if sent and ent['label'] not in self.stoplist]\n",
    "\n",
    "        return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "4657049a-07b7-449c-9b8f-550daa4c4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    \n",
    "    def __init__(self, vectorizer, midas_clf, \n",
    "                 entity_clf, entity_checker, \n",
    "                 responses, responses_vecs):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.midas_clf = midas_clf\n",
    "        self.entity_clf = entity_clf\n",
    "        self.entity_checker = entity_checker\n",
    "        self.id2midas = list(vectorizer.midas2id)\n",
    "        self.id2entity = list(vectorizer.entity2id)\n",
    "        self.responses = responses\n",
    "        self.responses_vecs = responses_vecs\n",
    "        \n",
    "    def infer_labels(self, sample) -> tuple:\n",
    "        \"\"\"\n",
    "        takes context, midas_vectors and annotated entities\n",
    "        and predicts midas label and entity for the next utterance\n",
    "        \"\"\"\n",
    "        vec = self.vectorizer.context_vector(\n",
    "            sample['previous_text'],\n",
    "            sample['midas_vectors'],\n",
    "            sample['previous_entities'])\n",
    "        # (n_features,) -> (1, n_features)\n",
    "        vec = vec[None,:]\n",
    "        midas_id = self.midas_clf.predict(vec)[0]\n",
    "        midas_pred = self.id2midas[midas_id]\n",
    "        \n",
    "        entities = self.entity_checker.get_context_entities(sample)\n",
    "        \n",
    "        entity_ids = [entity[2] for sent in entities if sent for entity in sent]\n",
    "        \n",
    "        entity_pred, entity_text = None, \"\"\n",
    "\n",
    "        if entity_ids:\n",
    "            # predict entity if there are of them in the context\n",
    "            entity_probas = self.entity_clf.predict_proba(vec)[0]\n",
    "            entity2proba = dict(zip(entity_ids, entity_probas[entity_ids]))\n",
    "            entity_id = max(entity2proba, key=entity2proba.get)\n",
    "            entity_pred = self.id2entity[entity_id]\n",
    "    \n",
    "        if entity_pred:\n",
    "            for sent in entities:\n",
    "                for ent in sent:\n",
    "                    if ent[0] == entity_pred:\n",
    "                        entity_text = ent[1]\n",
    "                \n",
    "            \n",
    "        return midas_pred, entity_pred, entity_text\n",
    "    \n",
    "    def get_candidate_ids(self, midas_label, entity_label) -> list:\n",
    "        \"\"\"\n",
    "        filters bank of responses and returns ids of candidates\n",
    "        meeting the midas_entity requirements\n",
    "        \"\"\"\n",
    "        midas_mask = self.responses.midas == midas_label\n",
    "        \n",
    "        if entity_label:\n",
    "            entity_mask = self.responses.entity == entity_label\n",
    "        else:\n",
    "            entity_mask = self.responses.entity.isnull()\n",
    "\n",
    "        candidate_ids = self.responses.index[(midas_mask) & (entity_mask)]\n",
    "        return candidate_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "0116060c-41fd-41fb-9c6c-95bb9288da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = Inference(\n",
    "    vectorizer=vectorizer,\n",
    "    midas_clf=rfc_midas,\n",
    "    entity_clf=entity_clf,\n",
    "    entity_checker=EntityChecker(Entity2ID),\n",
    "    responses=responses,\n",
    "    responses_vecs=responses_vecs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "1c112882-324d-4a59-a76f-8b57daca8fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Must be Las Vegas... Did you know Elmo is the only non-human to testify to the US congress?\n",
      "How is that possible lol. the congress is a circus at times\n",
      "Yep. I didn't know the Rep's and Dem's played a baseball game every year since 1909.\n",
      "\n",
      "\n",
      "in 1976 PERSON was a co-founder.\n",
      "in 1976 ELMO was a co-founder.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How is that possible lol. the congress is a circus at times\n",
      "Yep. I didn't know the Rep's and Dem's played a baseball game every year since 1909.\n",
      "So they also have a rivalry in sports. do you know which side is currently the winner?\n",
      "\n",
      "\n",
      "i don't really...know too much about SPORT.\n",
      "i don't really...know too much about BASEBALL.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yep. I didn't know the Rep's and Dem's played a baseball game every year since 1909.\n",
      "So they also have a rivalry in sports. do you know which side is currently the winner?\n",
      "The GOP is ahead 3 games. I can't believe Norway donated $1B to save the Amazon rainforest.  Good for them!\n",
      "\n",
      "\n",
      "i wonder if it did all the SPORT for them?\n",
      "i wonder if it did all the BASEBALL for them?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hello, back already? That was quick!\n",
      "Yes, luckily our office is just down the street.\n",
      "Great. I shall also need a copy of your own ID and the Warrant Letter, you can fill in one here.\n",
      "\n",
      "\n",
      "please fill out this form, and then take your parcel to the counter on my left-hand side.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes, luckily our office is just down the street.\n",
      "Great. I shall also need a copy of your own ID and the Warrant Letter, you can fill in one here.\n",
      "Oh, I see.\n",
      "\n",
      "\n",
      "please fill out this form, and then take your parcel to the counter on my left-hand side.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Do you like theater? I heard the Round House announced a 14 million dollar restoration plan.\n",
      "I do. just a little expensive for me to do it often.  I see that they are doing it mainly to break down the separation between audiences and artists.  I wonder if they looking to go more like a ancient Rome type play\n",
      "Maybe. I think they had some sound and sightline issues they needed to fix.\n",
      "\n",
      "\n",
      "i agree and also that shooting in LOCATION.\n",
      "i agree and also that shooting in ROUND HOUSE.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I do. just a little expensive for me to do it often.  I see that they are doing it mainly to break down the separation between audiences and artists.  I wonder if they looking to go more like a ancient Rome type play\n",
      "Maybe. I think they had some sound and sightline issues they needed to fix.\n",
      "I like the idea that they are bringing people on as full-time resident educators. and rotating new residents every 2 years.  It should make for a nice educational opportunity for aspiring theater people.\n",
      "\n",
      "\n",
      "i would love to spend a DURATION there and unwind.\n",
      "i would love to spend a 2 YEARS there and unwind.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Maybe. I think they had some sound and sightline issues they needed to fix.\n",
      "I like the idea that they are bringing people on as full-time resident educators. and rotating new residents every 2 years.  It should make for a nice educational opportunity for aspiring theater people.\n",
      "Yes. and they have funding now for 30 new works.\n",
      "\n",
      "\n",
      "that's pretty impressive considering it's been on for DURATION.\n",
      "that's pretty impressive considering it's been on for 2 YEARS.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I like the idea that they are bringing people on as full-time resident educators. and rotating new residents every 2 years.  It should make for a nice educational opportunity for aspiring theater people.\n",
      "Yes. and they have funding now for 30 new works.\n",
      "It's nice to hear something that is not technology based still making an impact.  I see that the renovation accomodate multiple stage and crowd configurations for audiences ranging 285 to 400 people.  I wish I was in the UK to get a chance to see it once renovated.\n",
      "\n",
      "\n",
      "it is nice to go and get one or NUMBER.\n",
      "it is nice to go and get one or 400.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes. and they have funding now for 30 new works.\n",
      "It's nice to hear something that is not technology based still making an impact.  I see that the renovation accomodate multiple stage and crowd configurations for audiences ranging 285 to 400 people.  I wish I was in the UK to get a chance to see it once renovated.\n",
      "Me too. Charcoalblue has done several things here in the US too. like Chicago's Steppenwolf Theater and Brooklyn's St. Ann's Warehouse.\n",
      "\n",
      "\n",
      "those are great shows i also like VIDEONAME.\n",
      "those are great shows i also like STEPPENWOLF.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It's nice to hear something that is not technology based still making an impact.  I see that the renovation accomodate multiple stage and crowd configurations for audiences ranging 285 to 400 people.  I wish I was in the UK to get a chance to see it once renovated.\n",
      "Me too. Charcoalblue has done several things here in the US too. like Chicago's Steppenwolf Theater and Brooklyn's St. Ann's Warehouse.\n",
      "I have been to Steppenwolf. it is a nice theater.  I was just reading how expensive it is to produce a play in the theater.  Construction can easily hit 9 million which seems like a lot for a play.  Makes me wonder how much they pay their artists\n",
      "\n",
      "\n",
      "for only NUMBER im sure a lot.\n",
      "for only 9 MILLION im sure a lot.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Me too. Charcoalblue has done several things here in the US too. like Chicago's Steppenwolf Theater and Brooklyn's St. Ann's Warehouse.\n",
      "I have been to Steppenwolf. it is a nice theater.  I was just reading how expensive it is to produce a play in the theater.  Construction can easily hit 9 million which seems like a lot for a play.  Makes me wonder how much they pay their artists\n",
      "That does seem like a lot. I guess the 14 million dollar initiative isn't as big a deal as I thought.\n",
      "\n",
      "\n",
      "i bet VIDEONAME made a lot of money out of the show.\n",
      "i bet STEPPENWOLF made a lot of money out of the show.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I have been to Steppenwolf. it is a nice theater.  I was just reading how expensive it is to produce a play in the theater.  Construction can easily hit 9 million which seems like a lot for a play.  Makes me wonder how much they pay their artists\n",
      "That does seem like a lot. I guess the 14 million dollar initiative isn't as big a deal as I thought.\n",
      "Switching gears a little. I was just reading that Jim Carrey doesn't make dramas any more because his belief system prevents him from playing in movies that don't project positivity.  I wonder how he worked around that system to be in Kick-Ass 2. not really positive there.\n",
      "\n",
      "\n",
      "i mean NUMBER isn't a small sum.\n",
      "i mean 14 MILLION DOLLAR isn't a small sum.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That does seem like a lot. I guess the 14 million dollar initiative isn't as big a deal as I thought.\n",
      "Switching gears a little. I was just reading that Jim Carrey doesn't make dramas any more because his belief system prevents him from playing in movies that don't project positivity.  I wonder how he worked around that system to be in Kick-Ass 2. not really positive there.\n",
      "I haven't seen that movie. I don't know too much about him. I watch House sometimes. Hugh Laurie was the highest paid actor for TV drama ever. earning 700.000 per episode.\n",
      "\n",
      "\n",
      "i think that NUMBER was worth it\n",
      "i think that 14 MILLION DOLLAR was worth it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Switching gears a little. I was just reading that Jim Carrey doesn't make dramas any more because his belief system prevents him from playing in movies that don't project positivity.  I wonder how he worked around that system to be in Kick-Ass 2. not really positive there.\n",
      "I haven't seen that movie. I don't know too much about him. I watch House sometimes. Hugh Laurie was the highest paid actor for TV drama ever. earning 700.000 per episode.\n",
      "I like him but that seems like a lot for just 1 episode.  Just seems like an insane amount of money when there are so many people who don't get to eat everyday.  I like that Michael Caine became an actor because all the pretty girls were in drama class.  He seems like that type of sauve. confident guy.\n",
      "\n",
      "\n",
      "i'm not surprised although PERSON is a good actor too.\n",
      "i'm not surprised although MICHAEL CAINE is a good actor too.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I haven't seen that movie. I don't know too much about him. I watch House sometimes. Hugh Laurie was the highest paid actor for TV drama ever. earning 700.000 per episode.\n",
      "I like him but that seems like a lot for just 1 episode.  Just seems like an insane amount of money when there are so many people who don't get to eat everyday.  I like that Michael Caine became an actor because all the pretty girls were in drama class.  He seems like that type of sauve. confident guy.\n",
      "It seems like a ridiculous amount for an episode. Jon Hamm taught drama at his high school. One of his students was that cute girl from the office.\n",
      "\n",
      "\n",
      "i'm not surprised although PERSON is a good actor too.\n",
      "i'm not surprised although JON HAMM is a good actor too.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I like him but that seems like a lot for just 1 episode.  Just seems like an insane amount of money when there are so many people who don't get to eat everyday.  I like that Michael Caine became an actor because all the pretty girls were in drama class.  He seems like that type of sauve. confident guy.\n",
      "It seems like a ridiculous amount for an episode. Jon Hamm taught drama at his high school. One of his students was that cute girl from the office.\n",
      "I find that interesting. goes to show that sometimes it helps to know someone.  I never knew that Disney almost shuttered their animation studio when Sleeping Beauty bombed at the Box Office\n",
      "\n",
      "\n",
      "the movie VIDEONAME went on to be very successful.\n",
      "the movie SLEEPING BEAUTY went on to be very successful.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It seems like a ridiculous amount for an episode. Jon Hamm taught drama at his high school. One of his students was that cute girl from the office.\n",
      "I find that interesting. goes to show that sometimes it helps to know someone.  I never knew that Disney almost shuttered their animation studio when Sleeping Beauty bombed at the Box Office\n",
      "Their \"Treasure Planet\" was one of the biggest flops in history.\n",
      "\n",
      "\n",
      "and also VIDEONAME that movie was a complete failure as well.\n",
      "and also TREASURE PLANET that movie was a complete failure as well.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I find that interesting. goes to show that sometimes it helps to know someone.  I never knew that Disney almost shuttered their animation studio when Sleeping Beauty bombed at the Box Office\n",
      "Their \"Treasure Planet\" was one of the biggest flops in history.\n",
      "Yeah that is a really weird movie.  Have you ever heard of Trojan Wars. cost 15 mil to make but has only earned $309 at the box office.  I wonder who got fired for that one.  Makes me interested in seeing it. lol\n",
      "\n",
      "\n",
      "the movie VIDEONAME went on to be very successful.\n",
      "the movie TREASURE PLANET went on to be very successful.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Their \"Treasure Planet\" was one of the biggest flops in history.\n",
      "Yeah that is a really weird movie.  Have you ever heard of Trojan Wars. cost 15 mil to make but has only earned $309 at the box office.  I wonder who got fired for that one.  Makes me interested in seeing it. lol\n",
      "Ha. can you imagine waiting to be called into your bosses' office after that? I would rather be resposible for silence of the lambs. that made like 270 million when it came out.\n",
      "\n",
      "\n",
      "and also VIDEONAME that movie was a complete failure as well.\n",
      "and also SILENCE OF THE LAMBS that movie was a complete failure as well.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yeah that is a really weird movie.  Have you ever heard of Trojan Wars. cost 15 mil to make but has only earned $309 at the box office.  I wonder who got fired for that one.  Makes me interested in seeing it. lol\n",
      "Ha. can you imagine waiting to be called into your bosses' office after that? I would rather be resposible for silence of the lambs. that made like 270 million when it came out.\n",
      "That is still one of the freaker movies I have seen.  Hopkins was great as the cannibal killer. I heard once that Jodie Foster's character was originally supposed to be played by Sandra Bullock\n",
      "\n",
      "\n",
      "PERSON made a ton of money in that role.\n",
      "SANDRA BULLOCK made a ton of money in that role.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ha. can you imagine waiting to be called into your bosses' office after that? I would rather be resposible for silence of the lambs. that made like 270 million when it came out.\n",
      "That is still one of the freaker movies I have seen.  Hopkins was great as the cannibal killer. I heard once that Jodie Foster's character was originally supposed to be played by Sandra Bullock\n",
      "Oh I didn't know that. I kind of like Sandra Bullock. we just watched Birdbox last night. I bet she would have been good for silence of the lambs.\n",
      "\n",
      "\n",
      "maybe if they had gotten it for the price of only $NUMBER.\n",
      "maybe if they had gotten it for the price of only $270 MILLION.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That is still one of the freaker movies I have seen.  Hopkins was great as the cannibal killer. I heard once that Jodie Foster's character was originally supposed to be played by Sandra Bullock\n",
      "Oh I didn't know that. I kind of like Sandra Bullock. we just watched Birdbox last night. I bet she would have been good for silence of the lambs.\n",
      "I am hoping to watch that this weekend. looks nice and freaky.  Reminds me of the movie A Quiet Place. check it out if you haven't seen it.  I just read that Pocahontas and Shakespeare were alive at the same time and died only a year apart.\n",
      "\n",
      "\n",
      "i did not know that and it's surprising when you think about PERSON and others.\n",
      "i did not know that and it's surprising when you think about SHAKESPEARE and others.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Oh I didn't know that. I kind of like Sandra Bullock. we just watched Birdbox last night. I bet she would have been good for silence of the lambs.\n",
      "I am hoping to watch that this weekend. looks nice and freaky.  Reminds me of the movie A Quiet Place. check it out if you haven't seen it.  I just read that Pocahontas and Shakespeare were alive at the same time and died only a year apart.\n",
      "I will definitely check it out. thanks. Did you know SHakespeare invented the names Miranda. Jessica and Olivia?\n",
      "\n",
      "\n",
      "yea if i remember correctly she plays the role of PERSON\n",
      "yea if i remember correctly she plays the role of SHAKESPEARE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Did you know about the wild birds in Australia and their pecularity?\n",
      "Can't say that I have. What about them?\n",
      "Well they mimic the sound of car alams. chainsaws and also cameras.\n",
      "\n",
      "\n",
      "i hear in LOCATION.\n",
      "i hear in AUSTRALIA.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Can't say that I have. What about them?\n",
      "Well they mimic the sound of car alams. chainsaws and also cameras.\n",
      "Oh. that sounds like the lyre birds. I guess I have heard of them. Birds are neat. The reason they can fly is thanks to their almost hollow skeleton.\n",
      "\n",
      "\n",
      "oh, VIDEONAME flies.\n",
      "oh, CHAINSAWS flies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Well they mimic the sound of car alams. chainsaws and also cameras.\n",
      "Oh. that sounds like the lyre birds. I guess I have heard of them. Birds are neat. The reason they can fly is thanks to their almost hollow skeleton.\n",
      "That must be really funny to listen at lol. you know that birds have bones that are so hollow they can even weigh more than their feathers\n",
      "\n",
      "\n",
      "that's a fun fact for characters of movie \"VIDEONAME\" :)\n",
      "that's a fun fact for characters of movie \"CHAINSAWS\" :)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Oh. that sounds like the lyre birds. I guess I have heard of them. Birds are neat. The reason they can fly is thanks to their almost hollow skeleton.\n",
      "That must be really funny to listen at lol. you know that birds have bones that are so hollow they can even weigh more than their feathers\n",
      "I believe it. especially when some of them are only two inches long. Can you imagine being so lightweight?\n",
      "\n",
      "\n",
      "NUMBER would go a long way.\n",
      "TWO would go a long way.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That must be really funny to listen at lol. you know that birds have bones that are so hollow they can even weigh more than their feathers\n",
      "I believe it. especially when some of them are only two inches long. Can you imagine being so lightweight?\n",
      "It would be graet to fly. thats something I can imagine. I bet the dodo bird was a magestic animal\n",
      "\n",
      "\n",
      "maybe if they had gotten it for the price of only $NUMBER.\n",
      "maybe if they had gotten it for the price of only $TWO.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I believe it. especially when some of them are only two inches long. Can you imagine being so lightweight?\n",
      "It would be graet to fly. thats something I can imagine. I bet the dodo bird was a magestic animal\n",
      "That depends. Do you consider pigeons majestic? Dodos are actually part of the pigeon family.\n",
      "\n",
      "\n",
      "right and to think they are worth NUMBER.\n",
      "right and to think they are worth TWO.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It would be graet to fly. thats something I can imagine. I bet the dodo bird was a magestic animal\n",
      "That depends. Do you consider pigeons majestic? Dodos are actually part of the pigeon family.\n",
      "No. not really. but Dodo seem to be a lot different than pigeons. do you know that some birds travel really long distances without sleeping!!\n",
      "\n",
      "\n",
      "that is another fact that i can't believe they do.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That depends. Do you consider pigeons majestic? Dodos are actually part of the pigeon family.\n",
      "No. not really. but Dodo seem to be a lot different than pigeons. do you know that some birds travel really long distances without sleeping!!\n",
      "Like when they migrate?\n",
      "\n",
      "\n",
      "no but i hear if they get stuck together there is no separating them\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No. not really. but Dodo seem to be a lot different than pigeons. do you know that some birds travel really long distances without sleeping!!\n",
      "Like when they migrate?\n",
      "Yeah exactly. when they migrate from Alska to the northern region of Mexico\n",
      "\n",
      "\n",
      "who knows maybe if we flew them to LOCATION.\n",
      "who knows maybe if we flew them to MEXICO.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Like when they migrate?\n",
      "Yeah exactly. when they migrate from Alska to the northern region of Mexico\n",
      "I'm glad that all birds don't migrate. Think of a 9 foot ostrich flying past in the sky!\n",
      "\n",
      "\n",
      "who knows maybe if we flew them to LOCATION.\n",
      "who knows maybe if we flew them to MEXICO.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yeah exactly. when they migrate from Alska to the northern region of Mexico\n",
      "I'm glad that all birds don't migrate. Think of a 9 foot ostrich flying past in the sky!\n",
      "That would be funny to watch. I like hummingbirds. spiders must hate them!\n",
      "\n",
      "\n",
      "i do worry about the ones in LOCATION.\n",
      "i do worry about the ones in MEXICO.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I'm glad that all birds don't migrate. Think of a 9 foot ostrich flying past in the sky!\n",
      "That would be funny to watch. I like hummingbirds. spiders must hate them!\n",
      "Why do you say that?\n",
      "\n",
      "\n",
      "i heard something about them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That would be funny to watch. I like hummingbirds. spiders must hate them!\n",
      "Why do you say that?\n",
      "Because hummingbirds steal spiderwebs from them. to construct their own nests. that would make me angry\n",
      "\n",
      "\n",
      "i would be so angry at myself.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why do you say that?\n",
      "Because hummingbirds steal spiderwebs from them. to construct their own nests. that would make me angry\n",
      "Understandably so! There's close to ten thousand different bird species. so those spiders had better look out.\n",
      "\n",
      "\n",
      "5,000 dozen is by no means a large order.\n",
      "5,000 dozen is by no means a large order.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Because hummingbirds steal spiderwebs from them. to construct their own nests. that would make me angry\n",
      "Understandably so! There's close to ten thousand different bird species. so those spiders had better look out.\n",
      "They must if they want to keep their webs lol. do you like Linkin Park?\n",
      "\n",
      "\n",
      "i'm more of a PERSON fan.\n",
      "i'm more of a LINKIN PARK fan.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Understandably so! There's close to ten thousand different bird species. so those spiders had better look out.\n",
      "They must if they want to keep their webs lol. do you like Linkin Park?\n",
      "Can't say that I'm much of a fan. I was sad to hear about Chester though.\n",
      "\n",
      "\n",
      "yeah. looks like they made a pretty bad decision as it is now worth NUMBER.\n",
      "yeah. looks like they made a pretty bad decision as it is now worth TEN THOUSAND.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "They must if they want to keep their webs lol. do you like Linkin Park?\n",
      "Can't say that I'm much of a fan. I was sad to hear about Chester though.\n",
      "Yeah that was really sad. maybe the 2001 tour got to him. they played 324 concerts that year alone\n",
      "\n",
      "\n",
      "PERSON may not have toured much.\n",
      "CHESTER may not have toured much.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Can't say that I'm much of a fan. I was sad to hear about Chester though.\n",
      "Yeah that was really sad. maybe the 2001 tour got to him. they played 324 concerts that year alone\n",
      "That's quite the tour. I wonder if all those stops finally took a toll on him.\n",
      "\n",
      "\n",
      "i wonder if they will be able to manage a tour for as long as PERSON did.\n",
      "i wonder if they will be able to manage a tour for as long as CHESTER did.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "do you watch the NBA?\n",
      "I sure do. DO you know who Reggie Miller is?\n",
      "Yeah the guy is a legend in the NBA but his sister was always better than him\n",
      "\n",
      "\n",
      "never heard of PERSON.\n",
      "never heard of REGGIE MILLER.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I sure do. DO you know who Reggie Miller is?\n",
      "Yeah the guy is a legend in the NBA but his sister was always better than him\n",
      "Yes she once scored 105 points during a game and broke 8 national records\n",
      "\n",
      "\n",
      "\n",
      "wow that is crazy because PERSON is great.\n",
      "wow that is crazy because REGGIE MILLER is great.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yeah the guy is a legend in the NBA but his sister was always better than him\n",
      "Yes she once scored 105 points during a game and broke 8 national records\n",
      "\n",
      "Yeah what a family with talented kids in sports\n",
      "\n",
      "\n",
      "ORGANIZATION must be so proud of that award too.\n",
      "NBA must be so proud of that award too.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes she once scored 105 points during a game and broke 8 national records\n",
      "\n",
      "Yeah what a family with talented kids in sports\n",
      "I know. Did you hear of the deal Iverson signed with Rebook in 2001?\n",
      "\n",
      "\n",
      "i didn't know that PERSON only had one recorded 3pt score throughout his whole career?\n",
      "i didn't know that IVERSON only had one recorded 3pt score throughout his whole career?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yeah what a family with talented kids in sports\n",
      "I know. Did you hear of the deal Iverson signed with Rebook in 2001?\n",
      "Yes he is making $800 000 a year from them until he is 55\n",
      "\n",
      "\n",
      "he's getting paid 800.000 a DURATION until 55 since 2001.\n",
      "he's getting paid 800.000 a YEAR until 55 since 2001.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I know. Did you hear of the deal Iverson signed with Rebook in 2001?\n",
      "Yes he is making $800 000 a year from them until he is 55\n",
      "Yes and then he gets a lump sum of $32 million\n",
      "\n",
      "\n",
      "\n",
      "i think he should have gotten DURATION and be forced to return every cent he collected.\n",
      "i think he should have gotten YEAR and be forced to return every cent he collected.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes he is making $800 000 a year from them until he is 55\n",
      "Yes and then he gets a lump sum of $32 million\n",
      "\n",
      "Yes it is safe to say that the guy is pretty much set for life\n",
      "\n",
      "\n",
      "he gets $800.000 a DURATION until age 55 and then a lump sum of $32.000.000!\n",
      "he gets $800.000 a YEAR until age 55 and then a lump sum of $32.000.000!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes and then he gets a lump sum of $32 million\n",
      "\n",
      "Yes it is safe to say that the guy is pretty much set for life\n",
      "Yeah. Do you like the granny style shot?\n",
      "\n",
      "\n",
      "yes and then after that he gets $NUMBER\n",
      "yes and then after that he gets $32 MILLION\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample, context_vec in zip(test_dataset[1000:1050], context_vecs[1000:1050]):\n",
    "    \n",
    "    midas_label, ent_label, ent_text = inference.infer_labels(sample)\n",
    "    candidate_ids = inference.get_candidate_ids(midas_label, ent_label)\n",
    "    candidate_vecs = inference.responses_vecs[candidate_ids]\n",
    "    scores = context_vec.dot(candidate_vecs.T)\n",
    "    candidate_pos = np.argmax(scores)\n",
    "    candidate_id = candidate_ids[candidate_pos]\n",
    "\n",
    "    context = [\" \".join(ut) for ut in sample['previous_text']]\n",
    "    for ut in context:\n",
    "        print(ut)\n",
    "    \n",
    "    print('\\n')        \n",
    "    text = inference.responses.at[candidate_id, 'text']\n",
    "    print(text)\n",
    "    if ent_text:\n",
    "        print(text.replace(ent_label.upper(), ent_text.upper()))\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "4787f5a7-e971-45af-9748-5df2a1904c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midas</th>\n",
       "      <th>entity</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statement</td>\n",
       "      <td>sport</td>\n",
       "      <td>i didn't even realize that he played any SPORT.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opinion</td>\n",
       "      <td>person</td>\n",
       "      <td>i head it was because PERSON was so dominant a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>statement</td>\n",
       "      <td>organization</td>\n",
       "      <td>usually though ORGANIZATION.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statement</td>\n",
       "      <td>location</td>\n",
       "      <td>i don't think anything is happening in LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>statement</td>\n",
       "      <td>softwareapplication</td>\n",
       "      <td>not unless SOFTWAREAPPLICATION publishes their...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       midas               entity  \\\n",
       "0  statement                sport   \n",
       "1    opinion               person   \n",
       "2  statement         organization   \n",
       "3  statement             location   \n",
       "4  statement  softwareapplication   \n",
       "\n",
       "                                                text  \n",
       "0    i didn't even realize that he played any SPORT.  \n",
       "1  i head it was because PERSON was so dominant a...  \n",
       "2                       usually though ORGANIZATION.  \n",
       "3  i don't think anything is happening in LOCATIO...  \n",
       "4  not unless SOFTWAREAPPLICATION publishes their...  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb84299-45c4-4b8e-ae2b-fc7e200eb377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
