{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7407b7f-6925-4b80-8041-5a14a5934e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "68a1c5dc-ceab-4ecc-8ece-6d9018911687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dial2seq():\n",
    "    \"\"\" \n",
    "    a class to transform dialogues into a sequence of utterances and labels\n",
    "    The sequence consists of n previous uterrances. \n",
    "    \n",
    "    There are no constraints\n",
    "    on number of entities or midas types in the them, however a sequence is \n",
    "    valid only when it is followed by an utterance with a single entity and\n",
    "    its midas label is not MISC or ANAPHOR.\n",
    "    \n",
    "    params:\n",
    "    path2data: str - a path to a json file with dialogues and their midas and cobot labels\n",
    "    seqlen: int - a number of utterances to use to predict next midas labels and entities\n",
    "    \"\"\"\n",
    "    def __init__(self, path2data: str, seqlen=2):\n",
    "        self.data = self.__load_data(path2data)\n",
    "        self.seqlen = seqlen\n",
    "\n",
    "        \n",
    "    def transform(self) -> list:\n",
    "        \"\"\"transforms dialogues into a set of sequences of size seqlen n+1\"\"\"\n",
    "        return [seq for dial in self for seq in self.__ngrammer(dial)]\n",
    "\n",
    "    \n",
    "    def __ngrammer(self, dialogue: list) -> list:\n",
    "        \"\"\" transforms a dialogue into a set of sequences (ngram style) \"\"\"\n",
    "        return [dialogue[i:i+self.seqlen+1] for i in range(len(dialogue)-(self.seqlen+1)+1)]\n",
    "        \n",
    "        \n",
    "    def __load_data(self, path: str) -> dict:\n",
    "        \"\"\" loads data from a json file \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"iterates over all the dialogues in the file \"\"\"\n",
    "        for dialogue in self.data.values():\n",
    "            yield dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "42c3b9d6-9dda-40c5-aba7-4c608b02ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer = Dial2seq('data/topicalchat_midas_cobot_entities_.json', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "23235657-03bd-4818-9ebe-aab09a1f58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = sequencer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ff77dac1-4a55-4415-9496-a184d8745c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162494"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4ab3e33-ac71-4260-be52-09dad5a65993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organization \n",
      "\n",
      "anaphor \n",
      "\n",
      "organization \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq in seqs[0:3]:\n",
    "    print(seq[-1]['ner']['response'][0]['label'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0272e592-31ed-4073-afb4-e0572ee5bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePreprocessor():\n",
    "    \"\"\" \n",
    "    preprocesses sequences\n",
    "    to filter only those that are relevant for the task\n",
    "    \n",
    "    params:\n",
    "    num_entities: int - maximum size of a last sentence in a sequence \n",
    "    in terms of number of cobot entities \n",
    "    \n",
    "    entities: list - if these cobot entities labels are in the last sentence\n",
    "    of a sequence, skip this seqence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_entities=1, entities: list = ['misc', 'anaphor']):\n",
    "        self.num_entities = num_entities\n",
    "        self.stoplist = entities\n",
    "        self.midas = Counter()\n",
    "        self.entities = Counter()\n",
    "        \n",
    "        \n",
    "    def transform(self, sequences: list) -> list:\n",
    "        \"\"\" extract necessary data from sequences \"\"\"\n",
    "        seqs = list()\n",
    "        \n",
    "        for seq in sequences:\n",
    "            sents, midas_labels, _, entities = self.preproc(seq[-1])\n",
    "            if not self.__is_valid(sents, midas_labels, entities):\n",
    "                continue\n",
    "            sample = self.__get_dict_entry(self.__shape_output(seq))\n",
    "            seqs.append(sample)\n",
    "            \n",
    "        return seqs\n",
    "    \n",
    "\n",
    "    def preproc(self, ut) -> tuple:\n",
    "        \"\"\" \n",
    "        opens up a single utterance to extract:\n",
    "        1. sentances (with nltk.sent_tokenize)\n",
    "        2. midas probability vector\n",
    "        3. all entities in this utterance\n",
    "        \n",
    "        returns tuple\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sents = sent_tokenize(ut['text'])\n",
    "        except IndexError:\n",
    "            # handles utterances with to much punctuation\n",
    "            sents = [ut['text']]\n",
    "        \n",
    "        midas_labels, midas_vectors = self.__get_midas(ut['midas_label'])\n",
    "                         \n",
    "        try:\n",
    "            entities = ut['ner']['response']\n",
    "        except KeyError:\n",
    "            # handles mislabelled samples\n",
    "            entities = []\n",
    "        \n",
    "        return sents, midas_labels, midas_vectors, entities\n",
    "\n",
    "    \n",
    "    def __is_valid(self, sents:list, midas_labels:list, entities:list) -> bool:\n",
    "        \"\"\"\n",
    "        checks if all the requirements for an utterance are met:\n",
    "        1. number of sents == number of midas_labels\n",
    "        2. an uterrance is one sentence, one midas label and \n",
    "        includes only one entity which is not in the stoplist\n",
    "        3. when an utterance has 2+ sentence, it will be valid if\n",
    "        the requirement 2 is applicable to the first sentence while\n",
    "        other sentences are omitted\n",
    "            \n",
    "        input:\n",
    "        sents: list - an utterance tokenized into sentences\n",
    "        midas_labels: list - midas_label per each sentence\n",
    "        entities: list of dicts - all entities in a given utterance (not mapped)\n",
    "        \n",
    "        output: bool\n",
    "        \"\"\"\n",
    "        if len(sents) != len(midas_labels) or not entities:\n",
    "            return False\n",
    "        \n",
    "        if len(sents) == 1 and len(entities) > 1:\n",
    "            return False\n",
    "        \n",
    "        sent_ents = self.__get_entities(sents[0], entities)\n",
    "        \n",
    "        if len(sent_ents) != 1:\n",
    "            return False\n",
    "\n",
    "        return sent_ents[0]['label'] not in self.stoplist\n",
    "    \n",
    "    \n",
    "    def __shape_output(self, seq: list) -> list:\n",
    "        \"\"\" shapes sequence in order to keep only the necessary data \"\"\"\n",
    "        \n",
    "        output = list()\n",
    "        \n",
    "        for ut in seq[:-1]:\n",
    "            try:\n",
    "                entities = ut['ner']['response']\n",
    "            except KeyError:\n",
    "                # handles mislabelled samples\n",
    "                # TODO: fix labelling\n",
    "                entities = []\n",
    "                \n",
    "            midas_labels, midas_vectors = self.__get_midas(ut['midas_label'])\n",
    "            \n",
    "            output.append((\n",
    "                # tuple of text, midas labels, entities for a utterance\n",
    "                ut['text'], midas_labels, midas_vectors, entities))\n",
    "\n",
    "        # preprocess last sentence in the sequence\n",
    "        sents, midas_labels, midas_vectors, entities = self.preproc(seq[-1])\n",
    "        output.append(\n",
    "            (sents[0], midas_labels[0:1], self.__get_entities(sents[0], entities)))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def __get_dict_entry(self, seq) -> dict:\n",
    "        \"\"\" creates a proper dict entry to dump into a file \"\"\"\n",
    "        entry = dict()\n",
    "        \n",
    "        for s in seq:\n",
    "            self.midas.update(s[1])\n",
    "            self.entities.update([label['label'] for label in s[-1]])\n",
    "\n",
    "        entry['previous_text'] = [s[0] for s in seq[:-1]]\n",
    "        entry['previous_midas'] = [s[1] for s in seq[:-1]]\n",
    "        entry['midas_vectors'] = [s[2] for s in seq[:-1]]\n",
    "        entry['previous_entities'] = [s[-1] for s in seq[:-1]]\n",
    "        entry['predict'] = {}\n",
    "        entry['predict']['text'] = seq[-1][0]\n",
    "        entry['predict']['midas'] = seq[-1][1][0]\n",
    "        entry['predict']['entity'] = seq[-1][2][0]\n",
    "        \n",
    "        return entry\n",
    "            \n",
    "        \n",
    "    def __get_midas(self, midas_labels: list) -> tuple:\n",
    "        \"\"\" \n",
    "        extracts midas labels with max value per each sentence in an utterance\n",
    "        and return a midas vector per each sentence\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        vectors = []\n",
    "        \n",
    "        for sample in midas_labels:\n",
    "            labels.append(max(sample, key=sample.get))\n",
    "            vectors.append(list(sample.values()))\n",
    "            \n",
    "        return labels, vectors\n",
    "    \n",
    "    \n",
    "    def __get_entities(self, sentence, entities) -> list:\n",
    "        \"\"\"\n",
    "        returns entities from a given list of entities\n",
    "        that are present in a given sentence\n",
    "        \"\"\"\n",
    "        return [ent for ent in entities if ent['text'] in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3d119ce2-45f6-4352-a2ac-d3aed5e99515",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = SequencePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2e9303b1-4c5f-4ea6-8818-732995e3c1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'previous_text': ['Yes it helped him smooth out his dance moves', 'Nice. Do you like Shakespeare?', 'Yes I do. Do you know that he popularized many phrases'], 'previous_midas': [['pos_answer'], ['comment', 'yes_no_question'], ['pos_answer', 'yes_no_question']], 'midas_vectors': [[[0.004489609505981207, 0.005659966263920069, 0.008467592298984528, 0.001957598840817809, 0.0032978374511003494, 0.0016933480510488153, 0.0015114849666133523, 0.001163744367659092, 0.011162664741277695, 0.002088370034471154, 0.8511312007904053, 0.10295984894037247, 0.004416705574840307]], [[0.33323583006858826, 0.0063044424168765545, 0.46820133924484253, 0.007547429762780666, 0.014540870673954487, 0.0016343685565516353, 0.003681134432554245, 0.0049273851327598095, 0.0934942215681076, 0.006875962950289249, 0.016408585011959076, 0.037494078278541565, 0.005654338281601667], [0.005343989469110966, 0.03268001973628998, 0.00530182896181941, 0.008314643055200577, 0.018734272569417953, 0.004701630212366581, 0.006959056947380304, 0.008828757330775261, 0.0016554039902985096, 0.00332073075696826, 0.017133507877588272, 0.004320445004850626, 0.8827057480812073]], [[0.0009711685124784708, 0.0010004272917285562, 0.0009106132201850414, 0.0003361883573234081, 0.0006613716832362115, 0.0012890041107311845, 0.00037702909321524203, 0.0003877524577546865, 0.000794330146163702, 0.0006375772063620389, 0.9901092052459717, 0.0018491200171411037, 0.0006762912380509079], [0.006708336994051933, 0.09151925891637802, 0.006718905176967382, 0.018919171765446663, 0.027768371626734734, 0.007621275261044502, 0.010829552076756954, 0.009083186276257038, 0.001821781974285841, 0.004542238544672728, 0.03815639764070511, 0.019157307222485542, 0.7571542263031006]]], 'previous_entities': [[{'text': 'it', 'label': 'anaphor'}, {'text': 'him', 'label': 'anaphor'}, {'text': 'dance moves', 'label': 'misc'}], [{'text': 'Shakespeare', 'label': 'person'}], [{'text': 'he', 'label': 'anaphor'}]], 'predict': {'text': 'Yes like good riddance, in my heart of hearts and such', 'midas': 'opinion', 'entity': {'text': 'good riddance', 'label': 'videoname'}}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq in preproc.transform(seqs[0:50]):\n",
    "    print(seq, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e17aaf5b-daad-4fac-901f-8aabcc7322f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10565"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = preproc.transform(seqs)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4fd1792-42ef-4d97-a80b-590c35db233d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'anaphor': 16742,\n",
       "         'misc': 38446,\n",
       "         'person': 11002,\n",
       "         'videoname': 5867,\n",
       "         'location': 6126,\n",
       "         'duration': 1566,\n",
       "         'number': 4912,\n",
       "         'position': 1129,\n",
       "         'date': 282,\n",
       "         'softwareapplication': 1431,\n",
       "         'year': 2266,\n",
       "         'vehicle': 562,\n",
       "         'wear': 354,\n",
       "         'genre': 3073,\n",
       "         'device': 4090,\n",
       "         'organization': 5312,\n",
       "         'event': 1136,\n",
       "         'sport': 2455,\n",
       "         'party': 324,\n",
       "         'ordinal': 34,\n",
       "         'bookname': 30,\n",
       "         'songname': 238,\n",
       "         'gamename': 320,\n",
       "         'sportteam': 1907,\n",
       "         'channelname': 335,\n",
       "         'sportrole': 250,\n",
       "         'venue': 13,\n",
       "         'albumname': 7})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "69eba087-db96-4fc9-9f9c-ad53a9320ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'previous_text': ['Do you like Star Wars ? The movie about Hans Solo just came out on Netflix. ',\n",
       "  'Is that the one where its about a young Han Solo?',\n",
       "  'Yeah it is the one, so Han Solo is not played by Harrison Ford, does it affect your opinion of the movie ?'],\n",
       " 'previous_midas': [['yes_no_question', 'statement'],\n",
       "  ['yes_no_question'],\n",
       "  ['yes_no_question']],\n",
       " 'midas_vectors': [[[0.005408135242760181,\n",
       "    0.03774670138955116,\n",
       "    0.004193521570414305,\n",
       "    0.005548159126192331,\n",
       "    0.01911454275250435,\n",
       "    0.0036064106971025467,\n",
       "    0.00729062594473362,\n",
       "    0.010243617929518223,\n",
       "    0.0017117718234658241,\n",
       "    0.0031786616891622543,\n",
       "    0.016034064814448357,\n",
       "    0.004651328548789024,\n",
       "    0.8812724947929382],\n",
       "   [0.0008340697386302054,\n",
       "    0.002748156199231744,\n",
       "    0.0018400404369458556,\n",
       "    0.0013773691607639194,\n",
       "    0.0016404872294515371,\n",
       "    0.0005165188922546804,\n",
       "    0.0008629477233625948,\n",
       "    0.000587132410146296,\n",
       "    0.040731094777584076,\n",
       "    0.0008753170259296894,\n",
       "    0.0008431581663899124,\n",
       "    0.9462572336196899,\n",
       "    0.00088641291949898]],\n",
       "  [[0.003861854085698724,\n",
       "    0.012315935455262661,\n",
       "    0.005729709751904011,\n",
       "    0.0032600853592157364,\n",
       "    0.008923898451030254,\n",
       "    0.0031420476734638214,\n",
       "    0.018601687625050545,\n",
       "    0.00914258137345314,\n",
       "    0.012623743154108524,\n",
       "    0.0029633648227900267,\n",
       "    0.005164223723113537,\n",
       "    0.027018390595912933,\n",
       "    0.8872524499893188]],\n",
       "  [[0.01092945784330368,\n",
       "    0.0104312589392066,\n",
       "    0.007987813092768192,\n",
       "    0.0026442869566380978,\n",
       "    0.014772586524486542,\n",
       "    0.0018285112455487251,\n",
       "    0.024696387350559235,\n",
       "    0.09648152440786362,\n",
       "    0.007600621320307255,\n",
       "    0.004781021736562252,\n",
       "    0.003662056988105178,\n",
       "    0.007306689862161875,\n",
       "    0.806877851486206]]],\n",
       " 'previous_entities': [[{'text': 'Star Wars', 'label': 'videoname'},\n",
       "   {'text': 'Hans Solo', 'label': 'person'},\n",
       "   {'text': 'Netflix', 'label': 'softwareapplication'}],\n",
       "  [{'text': 'Han Solo', 'label': 'person'}],\n",
       "  [{'text': 'it', 'label': 'anaphor'},\n",
       "   {'text': 'Han Solo', 'label': 'person'},\n",
       "   {'text': 'Harrison Ford', 'label': 'person'}]],\n",
       " 'predict': {'text': 'Someone else can fill Harrison Fords shoes?',\n",
       "  'midas': 'statement',\n",
       "  'entity': {'text': 'Harrison Fords', 'label': 'person'}}}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d1f15bc5-bcdc-4f31-b00f-f820432b5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1adc874a-9e74-4012-90cf-fc3909e8e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dict()\n",
    "\n",
    "labels['midas2id'] = {label: i for i, label in enumerate(preproc.midas.keys())}\n",
    "labels['entities2id'] = {label: i for i, label in enumerate(preproc.entities.keys())}\n",
    "\n",
    "with open('data/labels.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(labels, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
