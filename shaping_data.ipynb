{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9863d095-2c51-49b6-9b00-45f0f045141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c614df-9301-4a9f-be56-ee1ad902b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/labels.json', 'r') as f:\n",
    "    labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b0211a7c-b3ea-43c3-9a6e-1a6ef8826264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path2data: str, entities2id: dict):\n",
    "        self.data = self.__load_data(path2data)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.entities2id = entities2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" shape each sample into a proper \"\"\"\n",
    "        sample = self.data[index]\n",
    "        \n",
    "        x_text = sample['previous_text']\n",
    "        x_midas = self.__norm_midas(sample['midas_vectors'])\n",
    "        x_entities = [[ent['label'] for ent in ut] for ut in sample['previous_entities']]\n",
    "        x_entities = self.__ohencode(x_entities)\n",
    "        \n",
    "        y_midas = self.data[index]['predict']['midas']\n",
    "        y_entity = self.data[index]['predict']['entity']['label']\n",
    "        \n",
    "        return (x_text, x_midas, x_entities), [y_midas, y_entity]\n",
    "    \n",
    "    def __load_data(self, path: str) -> dict:\n",
    "        \"\"\" loads data from a json file \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def __norm_midas(self, midas_vectors: list) -> np.array:\n",
    "        \"\"\" \n",
    "        takes midas vectors of all sentences in the utterance\n",
    "        averages them and then applies softmax\n",
    "        \"\"\"\n",
    "        vecs = np.zeros((len(midas_vectors), 13))\n",
    "        \n",
    "        for i, vec in enumerate(midas_vectors):\n",
    "            # calc mean probability per each midas labels\n",
    "            vecs[i] = np.mean(np.array(vec), axis=0)\n",
    "        \n",
    "        # return normalized\n",
    "        return self.softmax(torch.Tensor(vecs))\n",
    "    \n",
    "    def __tokenize(self, texts) -> list:\n",
    "        \"\"\" transform list of strings into a list of list of tokens using spaCy \"\"\"\n",
    "        return [[token.lower_ for token in self.tokenizer(ut)] for ut in texts]\n",
    "    \n",
    "    def __ohencode(self, entities) -> torch.Tensor:\n",
    "        \"\"\" one-hot encoding of entities per each sample \"\"\"\n",
    "        ohe_vec = np.zeros((len(entities), len(self.entities2id)))\n",
    "        \n",
    "        for i, ut in enumerate(entities):\n",
    "            for ent in ut:\n",
    "                ohe_vec[i][self.entities2id[ent]] += 1\n",
    "                \n",
    "        return torch.Tensor(ohe_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "47bd6109-0fa1-441a-b8eb-a1bc2b28e7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10565"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SkillDataset('data/dataset.json', labels['entities2id'])\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "09f551cd-69f6-47e7-a60c-0212e9736693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's something else, that's for sure.\", \"In 1934 I see that North Dakota's governor declared martial law and seceded from the US.  I'm learning all kinds of new things today.\", 'Wow! Well, Texas elected their first female Governor, the second in the nation, before some states even ratified the 19th amendment. '] \n",
      "\n",
      "tensor([[0.0695, 0.0704, 0.0703, 0.0697, 0.0696, 0.0699, 0.0699, 0.0695, 0.1556,\n",
      "         0.0695, 0.0700, 0.0767, 0.0696],\n",
      "        [0.0690, 0.0694, 0.0694, 0.0694, 0.0691, 0.0691, 0.0691, 0.0690, 0.0748,\n",
      "         0.0690, 0.0691, 0.1646, 0.0690],\n",
      "        [0.0860, 0.0713, 0.0821, 0.0738, 0.0716, 0.0708, 0.0709, 0.0709, 0.0748,\n",
      "         0.0708, 0.0711, 0.1150, 0.0709]]) \n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 2., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 3., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) \n",
      "\n",
      "['opinion', 'location'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[36][0][0], '\\n') # x_text\n",
    "print(dataset[36][0][1], '\\n') # x_midas\n",
    "print(dataset[36][0][2], '\\n') # x_entitites\n",
    "print(dataset[36][1], '\\n') # y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a122d77d-c542-4bae-b04e-42c49079aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, test_size], \n",
    "    # fix the generator for reproducible results\n",
    "    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "6591f56b-64f5-4b51-9769-0d2ac41db76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8452, 2113)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8f1bbf9c-dc79-4022-97d8-3f57068f97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator():\n",
    "    \"\"\"\n",
    "    A class to preprocess batch to pass it to the model\n",
    "    \n",
    "    params:\n",
    "    tokenizer: Spacy tokenizer\n",
    "    vectorizer: sklearn TfIdfVectorizer (pretrained)\n",
    "    encoder: sklearn OneHotEncoder with preloaded categories\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer= None, vectorizer=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def collate_fn(self, batch) -> tuple:\n",
    "        \"\"\" preprocess batch for the model \"\"\"\n",
    "        return batch\n",
    "    \n",
    "collator = Collator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "db30d9a6-9a7e-48f6-8a07-6255e54d5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=2, \n",
    "    shuffle=False, collate_fn=collator.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b8e200d1-3ae6-4010-9922-c51b3b4c9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "37758299-72ea-4e98-998c-f07e4d85562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((['yeah i like to watch her play. Polo shirts were originally invented for tennis by famous player rene \"the crocodile\" lacoste. i love their colgne for men',\n",
       "    'Intresting, the longest match played in a polo shirt was 22 hours',\n",
       "    'yeah if i recall it went over three days or so'],\n",
       "   tensor([[0.0698, 0.0701, 0.0699, 0.0698, 0.0699, 0.0698, 0.0699, 0.0698, 0.1363,\n",
       "            0.0698, 0.0700, 0.0949, 0.0699],\n",
       "           [0.0695, 0.0697, 0.0698, 0.0695, 0.0695, 0.0694, 0.0695, 0.0694, 0.0819,\n",
       "            0.0694, 0.0694, 0.1537, 0.0694],\n",
       "           [0.0684, 0.0684, 0.0686, 0.0683, 0.0684, 0.0683, 0.0683, 0.0683, 0.0699,\n",
       "            0.0684, 0.0684, 0.1779, 0.0683]]),\n",
       "   tensor([[2., 2., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 2., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       "  ['statement', 'duration']),\n",
       " ((['I think they know so many people are on facebook and who knows, maybe facebook made a profit along  with them.',\n",
       "    \"It's hard to say. I guess BK and FB could work together. I would probably friend people just to unfriend them. \",\n",
       "    \"Yeah, I've seen some people with thousands of friends, most of which are complete strangers to them so those could be unfriended.  Facebook makes money from advertisements so the more BK advertised their promotion the more facebook made.\"],\n",
       "   tensor([[0.0686, 0.0690, 0.0688, 0.0686, 0.0687, 0.0686, 0.0688, 0.0687, 0.1723,\n",
       "            0.0686, 0.0687, 0.0719, 0.0687],\n",
       "           [0.0693, 0.0696, 0.0732, 0.0695, 0.0693, 0.0695, 0.0694, 0.0693, 0.1613,\n",
       "            0.0700, 0.0696, 0.0707, 0.0693],\n",
       "           [0.0701, 0.0708, 0.0703, 0.0702, 0.0702, 0.0701, 0.0703, 0.0702, 0.1077,\n",
       "            0.0702, 0.0703, 0.1194, 0.0702]]),\n",
       "   tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [2., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 2., 0., 4., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       "  ['opinion', 'organization'])]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7143e-5709-4ad7-a078-f0e094033ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0dff4-4712-451a-9498-8a63c02733a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
